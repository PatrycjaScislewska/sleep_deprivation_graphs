{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import numpy as np\n",
    "from networkx.algorithms import community\n",
    "import random\n",
    "import copy\n",
    "import scipy as sp\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import ttest_1samp\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "\n",
    "from statsmodels.formula.api import mixedlm\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from itertools import combinations\n",
    "import statsmodels.stats.multitest as smm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "\n",
    "import scipy.optimize as opt\n",
    "import sklearn as sk\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from matplotlib import ticker\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "from netplotbrain import plot\n",
    "\n",
    "from multipy.fwer import bonferroni\n",
    "from multipy.fdr import lsu\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this code, we will use the adjacency matrix and calculate nodal graph metrics (e.g., closeness, degree, betweenness).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load coordinates once - the coordinates are always the same, because they are obtained directly from the atlas\n",
    "coord_path = \"/path/to/coordinates/from/AAL/atlas.txt\"\n",
    "coord = pd.read_csv(coord_path, sep=\"\\s+\", header=None).to_numpy()\n",
    "pos = {k: (coord[k, 1], coord[k, 2]) for k in range(89)} \n",
    "\n",
    "#loop through all sessions and subjects\n",
    "base_path = \"/path/to/mai/folder/with/data\"\n",
    "sessions = [\"ses-1\", \"ses-2\", \"ses-3\"]\n",
    "\n",
    "for ses in sessions:\n",
    "    ses_path = os.path.join(base_path, ses)\n",
    "    subjects = glob.glob(os.path.join(ses_path, \"sub-*\"))\n",
    "\n",
    "    for sub_path in subjects:\n",
    "        sub = os.path.basename(sub_path)  # e.g., \"sub-01\"\n",
    "        graph_path = os.path.join(sub_path, \"Graphs/wAALours\")\n",
    "        \n",
    "        #only match files ending with \"_400.txt\" â€” this ensures we use adjacency matrices containing 400 edges.\n",
    "        #you can modify the number (e.g., \"_200.txt\", \"_600.txt\") to explore matrices with a different number of edges.\n",
    "        adj_files = glob.glob(os.path.join(graph_path, \"*_400.txt\"))\n",
    "\n",
    "        for adj_file in adj_files:\n",
    "            try:\n",
    "                #load adjacency matrix\n",
    "                df = pd.read_csv(adj_file, sep=\"\\s+\", header=None)\n",
    "                X = df.to_numpy()\n",
    "                G = nx.from_numpy_array(X)\n",
    "\n",
    "                #plot\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                nx.draw(G, pos, node_size=50, with_labels=False)\n",
    "\n",
    "                #output directory\n",
    "                metrics_dir = os.path.join(graph_path, \"graph_metrics\")\n",
    "                os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "                #file names follow the pattern \"ses-1_sub-01_Adj_mat_..._400.txt\".\n",
    "                #including \"ses\" (session) and \"sub\" (subject) helps verify that each file corresponds to the correct participant and session.\n",
    "                file_id = f\"{ses}_{sub}_{os.path.splitext(os.path.basename(adj_file))[0]}\"\n",
    "\n",
    "                #paths\n",
    "                image_path = os.path.join(metrics_dir, f\"{file_id}.png\")\n",
    "                metrics_path = os.path.join(metrics_dir, f\"{file_id}_metrics.csv\")\n",
    "\n",
    "                plt.savefig(image_path)\n",
    "                plt.close()\n",
    "\n",
    "                #compute metrics using NetworkX\n",
    "                closeness = nx.closeness_centrality(G)\n",
    "                betweenness = nx.betweenness_centrality(G)\n",
    "                clustering = nx.clustering(G)\n",
    "                degree = nx.degree_centrality(G)\n",
    "\n",
    "                #save to csv\n",
    "                metrics_df = pd.DataFrame({\n",
    "                    \"Node\": list(closeness.keys()),\n",
    "                    \"Closeness\": list(closeness.values()),\n",
    "                    \"Betweenness\": list(betweenness.values()),\n",
    "                    \"Clustering\": list(clustering.values()),\n",
    "                    \"Degree\": list(degree.values())\n",
    "                })\n",
    "                metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "                print(f\"Processed: {file_id}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {adj_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this code, we will use the adjacency matrix and calculate global graph metrics (e.g., global efficiency).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLOBAL EFFICIENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sessions\n",
    "session_labels = {\n",
    "    \"ses-1\": \"baseline\",\n",
    "    \"ses-2\": \"acute\",\n",
    "    \"ses-3\": \"chronic\"\n",
    "}\n",
    "\n",
    "base_path = \"/path/to/mai/folder/with/data\"\n",
    "\n",
    "#result list\n",
    "efficiency_results = []\n",
    "\n",
    "#loop through each session and subject\n",
    "for ses_id in session_labels.keys():\n",
    "    ses_path = os.path.join(base_path, ses_id)\n",
    "    subject_paths = glob.glob(os.path.join(ses_path, \"sub-*\"))\n",
    "\n",
    "    for sub_path in subject_paths:\n",
    "        sub_id = os.path.basename(sub_path)\n",
    "        adj_path = os.path.join(sub_path, \"Graphs/wAALours\", \"*cost_400.txt\")\n",
    "        adj_files = glob.glob(adj_path)\n",
    "\n",
    "        for adj_file in adj_files:\n",
    "            try:\n",
    "                df = pd.read_csv(adj_file, sep=\"\\s+\", header=None)\n",
    "                X = df.to_numpy()\n",
    "                G = nx.from_numpy_array(X)\n",
    "                efficiency = nx.global_efficiency(G)\n",
    "\n",
    "                efficiency_results.append({\n",
    "                    \"subject_id\": sub_id,\n",
    "                    \"session_id\": ses_id,\n",
    "                    \"global_efficiency\": efficiency\n",
    "                })\n",
    "\n",
    "                print(f\"{sub_id} | {ses_id} | {efficiency:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {adj_file}: {e}\")\n",
    "\n",
    "#convert to df\n",
    "df_out = pd.DataFrame(efficiency_results)\n",
    "\n",
    "#pivot the dataframe so that each session becomes a column, subject_id forms the rows,\n",
    "#session_id serves as the column names, and global_efficiency values fill the table.\n",
    "glob_eff = df_out.pivot(index=\"subject_id\", columns=\"session_id\", values=\"global_efficiency\").reset_index()\n",
    "glob_eff.columns.name = None  # remove column index name, e.g. \"session_id\"\"\n",
    "\n",
    "#save to CSV\n",
    "output_file = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/global_efficiency_all_subjects.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "glob_eff.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVERAGE CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sessions\n",
    "session_labels = {\n",
    "    \"ses-1\": \"baseline\",\n",
    "    \"ses-2\": \"acute\",\n",
    "    \"ses-3\": \"chronic\"\n",
    "}\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "\n",
    "clustering_results = []\n",
    "\n",
    "#loop through each session and subject\n",
    "for ses_id in session_labels.keys():\n",
    "    ses_path = os.path.join(base_path, ses_id)\n",
    "    subject_paths = glob.glob(os.path.join(ses_path, \"sub-*\"))\n",
    "\n",
    "    for sub_path in subject_paths:\n",
    "        sub_id = os.path.basename(sub_path)\n",
    "        adj_path = os.path.join(sub_path, \"Graphs/wAALours\", \"*cost_400.txt\")\n",
    "        adj_files = glob.glob(adj_path)\n",
    "\n",
    "        for adj_file in adj_files:\n",
    "            try:\n",
    "                df = pd.read_csv(adj_file, sep=\"\\s+\", header=None)\n",
    "                X = df.to_numpy()\n",
    "                G = nx.from_numpy_array(X)\n",
    "                avg_clustering = nx.average_clustering(G)\n",
    "\n",
    "                clustering_results.append({\n",
    "                    \"subject_id\": sub_id,\n",
    "                    \"session_id\": ses_id,\n",
    "                    \"average_clustering\": avg_clustering\n",
    "                })\n",
    "\n",
    "                print(f\"{sub_id} | {ses_id}: {avg_clustering:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {adj_file}: {e}\")\n",
    "\n",
    "#convert to df\n",
    "df_out = pd.DataFrame(clustering_results)\n",
    "\n",
    "#pivot the dataframe so that each session becomes a column, subject_id forms the rows,\n",
    "#session_id serves as the column names, and global_efficiency values fill the table.\n",
    "clustering_df = df_out.pivot(index=\"subject_id\", columns=\"session_id\", values=\"average_clustering\").reset_index()\n",
    "clustering_df.columns.name = None  # remove column index name\n",
    "\n",
    "#save to CSV\n",
    "output_file = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/average_clustering_all_subjects.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "clustering_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATH LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-14 | ses-1: 3.2171\n",
      "sub-25 | ses-1: 3.2393\n",
      "sub-24 | ses-1: 2.8874\n",
      "sub-23 | ses-1: 3.4339\n",
      "sub-12 | ses-1: 3.0600\n",
      "sub-30 | ses-1: 3.2439\n",
      "sub-08 | ses-1: 3.5776\n",
      "sub-01 | ses-1: 2.9816\n",
      "sub-06 | ses-1: 2.9198\n",
      "sub-07 | ses-1: 3.1606\n",
      "sub-09 | ses-1: 3.4752\n",
      "sub-31 | ses-1: 2.9732\n",
      "sub-28 | ses-1: 2.8131\n",
      "sub-10 | ses-1: 2.9778\n",
      "sub-26 | ses-1: 3.3358\n",
      "sub-19 | ses-1: 3.0434\n",
      "sub-21 | ses-1: 3.0687\n",
      "sub-20 | ses-1: 3.2329\n",
      "sub-27 | ses-1: 3.0013\n",
      "sub-18 | ses-1: 3.2661\n",
      "sub-16 | ses-1: 3.1187\n",
      "sub-29 | ses-1: 3.2684\n",
      "sub-34 | ses-1: 3.1527\n",
      "sub-33 | ses-1: 2.9459\n",
      "sub-02 | ses-1: 2.8521\n",
      "sub-03 | ses-1: 3.8859\n",
      "sub-04 | ses-1: 3.0940\n",
      "sub-32 | ses-1: 3.1272\n",
      "sub-14 | ses-2: 4.0403\n",
      "sub-25 | ses-2: 3.1793\n",
      "sub-24 | ses-2: 3.4982\n",
      "sub-23 | ses-2: 3.4505\n",
      "sub-12 | ses-2: 3.1463\n",
      "sub-30 | ses-2: 3.5278\n",
      "sub-08 | ses-2: 2.9806\n",
      "sub-01 | ses-2: 2.9229\n",
      "sub-06 | ses-2: 3.2280\n",
      "sub-07 | ses-2: 3.1129\n",
      "sub-09 | ses-2: 3.0669\n",
      "sub-31 | ses-2: 3.9763\n",
      "sub-28 | ses-2: 2.9356\n",
      "sub-10 | ses-2: 3.0309\n",
      "sub-26 | ses-2: 2.7528\n",
      "sub-19 | ses-2: 3.0235\n",
      "sub-21 | ses-2: 2.8922\n",
      "sub-20 | ses-2: 3.7135\n",
      "sub-27 | ses-2: 3.0268\n",
      "sub-18 | ses-2: 2.8253\n",
      "sub-16 | ses-2: 3.4663\n",
      "sub-29 | ses-2: 3.5097\n",
      "sub-34 | ses-2: 3.2676\n",
      "sub-33 | ses-2: 3.0825\n",
      "sub-02 | ses-2: 3.0089\n",
      "sub-03 | ses-2: 3.3177\n",
      "sub-04 | ses-2: 3.1987\n",
      "sub-32 | ses-2: 3.4244\n",
      "sub-14 | ses-3: 3.5712\n",
      "sub-25 | ses-3: 3.3059\n",
      "sub-24 | ses-3: 3.1098\n",
      "sub-23 | ses-3: 3.1895\n",
      "sub-12 | ses-3: 3.1992\n",
      "sub-30 | ses-3: 3.2906\n",
      "sub-08 | ses-3: 2.9612\n",
      "sub-01 | ses-3: 3.2957\n",
      "sub-06 | ses-3: 3.0074\n",
      "sub-07 | ses-3: 2.9420\n",
      "sub-09 | ses-3: 3.1300\n",
      "sub-31 | ses-3: 3.0993\n",
      "sub-28 | ses-3: 2.8910\n",
      "sub-10 | ses-3: 3.3447\n",
      "sub-26 | ses-3: 3.1834\n",
      "sub-19 | ses-3: 3.0123\n",
      "sub-21 | ses-3: 2.9428\n",
      "sub-20 | ses-3: 3.5237\n",
      "sub-27 | ses-3: 3.1433\n",
      "sub-18 | ses-3: 2.9658\n",
      "sub-16 | ses-3: 3.5140\n",
      "sub-29 | ses-3: 3.3996\n",
      "sub-34 | ses-3: 3.3527\n",
      "sub-33 | ses-3: 2.9017\n",
      "sub-02 | ses-3: 2.7543\n",
      "sub-03 | ses-3: 3.2354\n",
      "sub-04 | ses-3: 3.1351\n",
      "sub-32 | ses-3: 2.7942\n",
      "Final results saved to: /Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/average_path_length_all_subjects.csv\n"
     ]
    }
   ],
   "source": [
    "#sessions\n",
    "session_labels = {\n",
    "    \"ses-1\": \"baseline\",\n",
    "    \"ses-2\": \"acute\",\n",
    "    \"ses-3\": \"chronic\"\n",
    "}\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "\n",
    "path_length_results = []\n",
    "\n",
    "#loop through each session and subject\n",
    "for ses_id in session_labels.keys():\n",
    "    ses_path = os.path.join(base_path, ses_id)\n",
    "    subject_paths = glob.glob(os.path.join(ses_path, \"sub-*\"))\n",
    "\n",
    "    for sub_path in subject_paths:\n",
    "        sub_id = os.path.basename(sub_path)\n",
    "        adj_path = os.path.join(sub_path, \"Graphs/wAALours\", \"*cost_400.txt\")\n",
    "        adj_files = glob.glob(adj_path)\n",
    "\n",
    "        for adj_file in adj_files:\n",
    "            try:\n",
    "                df = pd.read_csv(adj_file, sep=\"\\s+\", header=None)\n",
    "                X = df.to_numpy()\n",
    "                G = nx.from_numpy_array(X)\n",
    "                if nx.is_connected(G):\n",
    "                    avg_path_length = nx.average_shortest_path_length(G)\n",
    "                else:\n",
    "                    largest_cc = max(nx.connected_components(G), key=len)\n",
    "                    G_largest_cc = G.subgraph(largest_cc)\n",
    "                    avg_path_length = nx.average_shortest_path_length(G_largest_cc)\n",
    "\n",
    "                path_length_results.append({\n",
    "                    \"subject_id\": sub_id,\n",
    "                    \"session_id\": ses_id,\n",
    "                    \"average_path_length\": avg_path_length\n",
    "                })\n",
    "\n",
    "                print(f\"{sub_id} | {ses_id}: {avg_path_length:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {adj_file}: {e}\")\n",
    "\n",
    "#convert to df\n",
    "df_out = pd.DataFrame(path_length_results)\n",
    "\n",
    "#pivot the dataframe so that each session becomes a column, subject_id forms the rows,\n",
    "#session_id serves as the column names, and global_efficiency values fill the table.\n",
    "path_length_df = df_out.pivot(index=\"subject_id\", columns=\"session_id\", values=\"average_path_length\").reset_index()\n",
    "path_length_df.columns.name = None  # remove column index name\n",
    "\n",
    "#save to CSV\n",
    "output_file = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/average_path_length_all_subjects.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "path_length_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODULARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sessions\n",
    "session_labels = {\n",
    "    \"ses-1\": \"baseline\",\n",
    "    \"ses-2\": \"acute\",\n",
    "    \"ses-3\": \"chronic\"\n",
    "}\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "\n",
    "modularity_results = []\n",
    "\n",
    "#loop through each session and subject\n",
    "for ses_id in session_labels.keys():\n",
    "    ses_path = os.path.join(base_path, ses_id)\n",
    "    subject_paths = glob.glob(os.path.join(ses_path, \"sub-*\"))\n",
    "\n",
    "    for sub_path in subject_paths:\n",
    "        sub_id = os.path.basename(sub_path)\n",
    "        adj_path = os.path.join(sub_path, \"Graphs/wAALours\", \"*cost_400.txt\")\n",
    "        adj_files = glob.glob(adj_path)\n",
    "\n",
    "        for adj_file in adj_files:\n",
    "            try:\n",
    "                df = pd.read_csv(adj_file, sep=\"\\s+\", header=None)\n",
    "                X = df.to_numpy()\n",
    "                G = nx.from_numpy_array(X)\n",
    "                communities = community.greedy_modularity_communities(G)\n",
    "                modularity = community.modularity(G, communities)\n",
    "\n",
    "                modularity_results.append({\n",
    "                    \"subject_id\": sub_id,\n",
    "                    \"session_id\": ses_id,\n",
    "                    \"modularity\": modularity\n",
    "                })\n",
    "\n",
    "                print(f\"{sub_id} | {ses_id}: {modularity:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {adj_file}: {e}\")\n",
    "\n",
    "#convert to df\n",
    "df_out = pd.DataFrame(modularity_results)\n",
    "\n",
    "#pivot the dataframe so that each session becomes a column, subject_id forms the rows,\n",
    "#session_id serves as the column names, and global_efficiency values fill the table.\n",
    "modularity_df = df_out.pivot(index=\"subject_id\", columns=\"session_id\", values=\"modularity\").reset_index()\n",
    "modularity_df.columns.name = None  # remove column index name\n",
    "\n",
    "#save to CSV\n",
    "output_file = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/modularity_all_subjects.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "modularity_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODULARITY, COMMUNITY DISTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "coord_path = os.path.join(base_path, \"ses-1\", \"coord_AALours.txt\")\n",
    "coord = pd.read_csv(coord_path, sep=\"\\s+\", header=None).to_numpy()\n",
    "\n",
    "session_labels = {\n",
    "    \"ses-1\": \"baseline\",\n",
    "    \"ses-2\": \"acute\",\n",
    "    \"ses-3\": \"chronic\"\n",
    "}\n",
    "\n",
    "\n",
    "community_summary = []\n",
    "\n",
    "# --- DISTANCE FUNCTIONS ---\n",
    "\n",
    "def compute_spatial_distances(communities, coord):\n",
    "    comm_centroids = []\n",
    "    for comm in communities:\n",
    "        comm_coords = coord[list(comm)]\n",
    "        centroid = comm_coords.mean(axis=0)\n",
    "        comm_centroids.append(centroid)\n",
    "    return cdist(comm_centroids, comm_centroids, metric='euclidean')\n",
    "\n",
    "def compute_graph_distances(G, communities):\n",
    "    n = len(communities)\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            paths = []\n",
    "            for node_i in communities[i]:\n",
    "                for node_j in communities[j]:\n",
    "                    try:\n",
    "                        length = nx.shortest_path_length(G, source=node_i, target=node_j)\n",
    "                        paths.append(length)\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "            avg_dist = np.mean(paths) if paths else np.nan\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = avg_dist\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "\n",
    "for ses_id in session_labels.keys():\n",
    "    ses_path = os.path.join(base_path, ses_id)\n",
    "    subject_paths = glob.glob(os.path.join(ses_path, \"sub-*\"))\n",
    "\n",
    "    for sub_path in subject_paths:\n",
    "        sub_id = os.path.basename(sub_path)\n",
    "        adj_path_pattern = os.path.join(sub_path, \"Graphs/wAALours\", \"*cost_400.txt\")\n",
    "        adj_files = glob.glob(adj_path_pattern)\n",
    "\n",
    "        for adj_file in adj_files:\n",
    "            try:\n",
    "                df = pd.read_csv(adj_file, sep=\"\\s+\", header=None)\n",
    "                adj_matrix = df.to_numpy()\n",
    "                G = nx.from_numpy_array(adj_matrix)\n",
    "                communities = community.greedy_modularity_communities(G)\n",
    "                modularity_value = community.modularity(G, communities)\n",
    "                num_communities = len(communities)\n",
    "\n",
    "                node_community_map = {node: idx for idx, comm in enumerate(communities) for node in comm}\n",
    "                cmap = cm.get_cmap('tab20', num_communities)\n",
    "                node_colors = [cmap(node_community_map[n]) for n in G.nodes()]\n",
    "\n",
    "                #views\n",
    "                views = {\n",
    "                    'sagittal': {k: (coord[k, 0], coord[k, 2]) for k in range(len(G.nodes))},\n",
    "                    'axial':    {k: (coord[k, 0], coord[k, 1]) for k in range(len(G.nodes))},\n",
    "                    'coronal':  {k: (coord[k, 1], coord[k, 2]) for k in range(len(G.nodes))}\n",
    "                }\n",
    "\n",
    "                #plot\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                for i, (view_name, pos) in enumerate(views.items()):\n",
    "                    nx.draw(G, pos, node_color=node_colors, with_labels=False, node_size=100, ax=axs[i])\n",
    "                    axs[i].set_title(f'{view_name.capitalize()} view')\n",
    "\n",
    "                adj_basename = os.path.basename(adj_file).replace(\".txt\", \"\")\n",
    "                fig.suptitle(f\"{sub_id} | {ses_id} | Community Structure\\n{adj_basename}\", fontsize=16)\n",
    "                plt.tight_layout()\n",
    "\n",
    "                output_dir = os.path.join(sub_path, \"Graphs/wAALours\", \"graph_metrics\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                plot_file = os.path.join(output_dir, f\"{adj_basename}_communities.png\")\n",
    "                plt.savefig(plot_file)\n",
    "                plt.close()\n",
    "\n",
    "                #distances\n",
    "                spatial_distances = compute_spatial_distances(communities, coord)\n",
    "                graph_distances = compute_graph_distances(G, communities)\n",
    "\n",
    "                #save matrices\n",
    "                spatial_file = os.path.join(output_dir, f\"{sub_id}_{ses_id}_communities_spatial_distances.csv\")\n",
    "                graph_file = os.path.join(output_dir, f\"{sub_id}_{ses_id}_communities_graph_distances.csv\")\n",
    "                pd.DataFrame(spatial_distances).to_csv(spatial_file, index=False)\n",
    "                pd.DataFrame(graph_distances).to_csv(graph_file, index=False)\n",
    "\n",
    "                #add to global summary\n",
    "                avg_spatial_dist = np.nanmean(spatial_distances[np.triu_indices(num_communities, k=1)])\n",
    "                avg_graph_dist = np.nanmean(graph_distances[np.triu_indices(num_communities, k=1)])\n",
    "                community_summary.append({\n",
    "                    \"subject_id\": sub_id,\n",
    "                    \"session_id\": ses_id,\n",
    "                    \"modularity\": modularity_value,\n",
    "                    \"num_communities\": num_communities,\n",
    "                    \"avg_spatial_distance\": avg_spatial_dist,\n",
    "                    \"avg_graph_distance\": avg_graph_dist\n",
    "                })\n",
    "\n",
    "                print(f\"{sub_id} {ses_id}: saved plot and metrics to {output_dir}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {adj_file}: {e}\")\n",
    "\n",
    "output_base_dir = os.path.join(base_path, \"graph_metrics\")\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "#convert summary to df\n",
    "summary_df = pd.DataFrame(community_summary)\n",
    "\n",
    "#create and save pivoted dfs\n",
    "modularity_df = summary_df.pivot(index=\"subject_id\", columns=\"session_id\", values=\"modularity\").reset_index()\n",
    "num_communities_df = summary_df.pivot(index=\"subject_id\", columns=\"session_id\", values=\"num_communities\").reset_index()\n",
    "avg_spatial_distance_df = summary_df.pivot(index=\"subject_id\", columns=\"session_id\", values=\"avg_spatial_distance\").reset_index()\n",
    "avg_graph_distance_df = summary_df.pivot(index=\"subject_id\", columns=\"session_id\", values=\"avg_graph_distance\").reset_index()\n",
    "\n",
    "\n",
    "for df in [modularity_df, num_communities_df, avg_spatial_distance_df, avg_graph_distance_df]:\n",
    "    df.columns.name = None\n",
    "\n",
    "#save to CSV\n",
    "modularity_df.to_csv(os.path.join(output_base_dir, \"modularity_all_subjects.csv\"), index=False)\n",
    "num_communities_df.to_csv(os.path.join(output_base_dir, \"num_communities_all_subjects.csv\"), index=False)\n",
    "avg_spatial_distance_df.to_csv(os.path.join(output_base_dir, \"avg_spatial_distance_all_subjects.csv\"), index=False)\n",
    "avg_graph_distance_df.to_csv(os.path.join(output_base_dir, \"avg_graph_distance_all_subjects.csv\"), index=False)\n",
    "\n",
    "print(f\"All community metrics saved to: {output_base_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical comparisons of global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_eff = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/global_efficiency_all_subjects.csv\")\n",
    "clustering_df = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/average_clustering_all_subjects.csv\")\n",
    "path_length_df = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/average_path_length_all_subjects.csv\")\n",
    "modularity_df = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/modularity_all_subjects.csv\")\n",
    "num_communities_df = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/num_communities_all_subjects.csv\")\n",
    "avg_spatial_distance_df = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/avg_spatial_distance_all_subjects.csv\")\n",
    "avg_graph_distance_df = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/avg_graph_distance_all_subjects.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics\"\n",
    "metrics = {\n",
    "    \"Global efficiency\": glob_eff,\n",
    "    \"Average clustering\": clustering_df,\n",
    "    \"Average path length\": path_length_df,\n",
    "    \"Modularity\": modularity_df,\n",
    "    \"Average graph distance\": avg_graph_distance_df #between communities\n",
    "}\n",
    "\n",
    "lmm_results = []\n",
    "\n",
    "#linear mixed model\n",
    "def run_lmm_with_ref(df_long, metric_name, ref_session):\n",
    "    #session have to be \"categories\", otherwise they would be treated as continous variables\n",
    "    df_long[\"session\"] = pd.Categorical(df_long[\"session\"], categories=[\"ses-1\", \"ses-2\", \"ses-3\"])\n",
    "    #by default LMM compares everything to ses-1 (alphabetic order), but we want to have the ses-2 vs ses-3 comparison as well, so here we will \n",
    "    # build a new category order with the reference session first. Reference session is defined lower in the loop\n",
    "    df_long[\"session\"] = df_long[\"session\"].cat.reorder_categories([ref_session, \n",
    "                                                                    *(s for s in [\"ses-1\", \"ses-2\", \"ses-3\"] if s != ref_session)])\n",
    "    try:\n",
    "        model = smf.mixedlm(\"value ~ session\", data=df_long, groups=df_long[\"subject_id\"])\n",
    "        result = model.fit()\n",
    "        conf_int = result.conf_int()  \n",
    "        print(f\"\\n{metric_name} (reference: {ref_session}):\\n\", result.summary())\n",
    "\n",
    "        for param in result.fe_params.index:\n",
    "            lmm_results.append({\n",
    "                \"Metric\": metric_name,\n",
    "                \"Reference Session\": ref_session,\n",
    "                \"Effect\": param,\n",
    "                \"Estimate\": result.fe_params[param],\n",
    "                \"CI Lower Bound\": conf_int.loc[param][0],\n",
    "                \"CI Upper Bound\": conf_int.loc[param][1],\n",
    "                \"p-value\": result.pvalues[param]\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model for {metric_name} with ref {ref_session}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "for metric_name, df_metric in metrics.items():\n",
    "    df_long = df_metric.melt(id_vars=\"subject_id\", \n",
    "                             var_name=\"session\", \n",
    "                             value_name=\"value\")\n",
    "\n",
    "    #run lmm with ses-1 as reference (default)\n",
    "    run_lmm_with_ref(df_long.copy(), metric_name, ref_session=\"ses-1\")\n",
    "\n",
    "    #run lmm with ses-2 as reference to get ses-3 vs ses-2 - this \"ref_session\" is used in our function \"run_lmm_with_ref\"\n",
    "    run_lmm_with_ref(df_long.copy(), metric_name, ref_session=\"ses-2\")\n",
    "\n",
    "\n",
    "#save\n",
    "lmm_df = pd.DataFrame(lmm_results)\n",
    "output_file = os.path.join(base_path, \"lmm_results_all_metrics_with_ref.csv\")\n",
    "lmm_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nLinear mixed model results saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# LMM results with CIs\n",
    "file_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/lmm_results_all_metrics_with_ref.csv\"\n",
    "lmm_df = pd.read_csv(file_path)\n",
    "\n",
    "selected_metrics = [\n",
    "    \"Global efficiency\",\n",
    "    \"Average clustering\",\n",
    "    \"Average path length\",\n",
    "    \"Modularity\",\n",
    "    \"Average graph distance\"\n",
    "]\n",
    "\n",
    "custom_titles = [\n",
    "    \"TSD vs RW\",   # ses-2 vs ses-1 (control)\n",
    "    \"CSR vs RW\", # ses-3 vs ses-1 (control)\n",
    "    \"CSR vs TSD\"     # ses-3 vs ses-2 (control)\n",
    "]\n",
    "\n",
    "\n",
    "#remove intercepts and filter metrics\n",
    "lmm_df = lmm_df[~lmm_df[\"Effect\"].str.contains(\"Intercept\")]\n",
    "lmm_df = lmm_df[lmm_df[\"Metric\"].isin(selected_metrics)].copy()\n",
    "\n",
    "#extract comparison info\n",
    "lmm_df[\"Compared Session\"] = lmm_df[\"Effect\"].str.extract(r'session\\[T\\.(.*?)\\]')\n",
    "lmm_df[\"Comparison\"] = lmm_df[\"Compared Session\"] + \" vs \" + lmm_df[\"Reference Session\"]\n",
    "\n",
    "#add new columns for FDR correction\n",
    "lmm_df[\"p-value (FDR)\"] = None\n",
    "lmm_df[\"Significant (FDR)\"] = None\n",
    "\n",
    "#FDR correction \n",
    "for (comp_ses, ref_ses) in [\n",
    "    (\"ses-2\", \"ses-1\"),\n",
    "    (\"ses-3\", \"ses-1\"),\n",
    "    (\"ses-3\", \"ses-2\")\n",
    "]:\n",
    "    mask = (lmm_df[\"Compared Session\"] == comp_ses) & (lmm_df[\"Reference Session\"] == ref_ses)\n",
    "    pvals = lmm_df.loc[mask, \"p-value\"].values\n",
    "\n",
    "    if len(pvals) > 0:\n",
    "        rejected, pvals_corrected, _, _ = multipletests(pvals, method='fdr_bh', alpha=0.05)\n",
    "\n",
    "        #corrected p-values and rejection flags - decisions\n",
    "        lmm_df.loc[mask, \"p-value (FDR)\"] = pvals_corrected\n",
    "        lmm_df.loc[mask, \"Significant (FDR)\"] = rejected\n",
    "\n",
    "#convert to correct types - numeric for p value and boolean for decision\n",
    "lmm_df[\"p-value (FDR)\"] = pd.to_numeric(lmm_df[\"p-value (FDR)\"])\n",
    "lmm_df[\"Significant (FDR)\"] = lmm_df[\"Significant (FDR)\"].astype(bool)\n",
    "\n",
    "#significance markers and colors\n",
    "def get_sig_marker_fdr(p):\n",
    "    if p <= 0.001: return '***'\n",
    "    elif p <= 0.01: return '**'\n",
    "    elif p <= 0.05: return '*'\n",
    "    elif p <= 0.1: return '#'\n",
    "    else: return ''\n",
    "\n",
    "def get_color(p):\n",
    "    if p < 0.05: return 'darkturquoise'\n",
    "    elif p < 0.1: return 'black'\n",
    "    else: return 'gray'\n",
    "\n",
    "lmm_df[\"Significance (FDR)\"] = lmm_df[\"p-value (FDR)\"].apply(get_sig_marker_fdr)\n",
    "lmm_df[\"Color\"] = lmm_df[\"p-value (FDR)\"].apply(get_color)\n",
    "\n",
    "#plot\n",
    "comparison_order = [\n",
    "    (\"ses-2\", \"ses-1\"),\n",
    "    (\"ses-3\", \"ses-1\"),\n",
    "    (\"ses-3\", \"ses-2\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for i, (comp_ses, ref_ses) in enumerate(comparison_order):\n",
    "    df_comp = lmm_df[(lmm_df[\"Compared Session\"] == comp_ses) & (lmm_df[\"Reference Session\"] == ref_ses)].copy()\n",
    "\n",
    "    metric_order = selected_metrics[::-1]\n",
    "    df_comp[\"Metric\"] = pd.Categorical(df_comp[\"Metric\"], categories=metric_order, ordered=True)\n",
    "    df_comp = df_comp.sort_values(\"Metric\")\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.axvline(x=0, color='gray', linestyle='--')\n",
    "    ax.set_xlim(-0.6, 0.6)\n",
    "\n",
    "    for _, row in df_comp.iterrows():\n",
    "        y = row[\"Metric\"]\n",
    "        x = row[\"Estimate\"]\n",
    "        x_low = row[\"CI Lower Bound\"]\n",
    "        x_high = row[\"CI Upper Bound\"]\n",
    "        color = row[\"Color\"]\n",
    "        sig = row[\"Significance (FDR)\"]\n",
    "        label = f'{x:.4f}{sig}'\n",
    "\n",
    "        ax.plot([x_low, x_high], [y, y], color=color, linewidth=4)\n",
    "        ax.plot(x, y, 'o', color=color, markersize=10)\n",
    "        ax.text(min(x_high + 0.03, 0.48), y, label, va='center', fontsize=13)\n",
    "\n",
    "    ax.set_title(custom_titles[i], fontsize=18)\n",
    "    ax.set_xlabel(\"Estimate\", fontsize=16)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Graph metric\", fontsize=16)\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# plt.suptitle(\"LMM coefficients and CI (FDR corrected)\", fontsize=15)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#save to CSV\n",
    "output_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/lmm_results_with_FDR.csv\"\n",
    "\n",
    "lmm_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpaired permutations (broken pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### permutations for checking the robustness of LMM for global metrics - broken pairs ##############\n",
    "\n",
    "#metrics\n",
    "metrics = {\n",
    "    \"Global efficiency\": glob_eff,\n",
    "    \"Average clustering\": clustering_df,\n",
    "    \"Average path length\": path_length_df,\n",
    "    \"Modularity\": modularity_df,\n",
    "    \"Average graph distance\": avg_graph_distance_df\n",
    "}\n",
    "\n",
    "n_iterations = 10000\n",
    "n_subjects = 28\n",
    "session_names = ['ses-1', 'ses-2', 'ses-3']\n",
    "session_pairs = [\n",
    "    ('ses-1', 'ses-2'),\n",
    "    ('ses-1', 'ses-3'),\n",
    "    ('ses-2', 'ses-3')\n",
    "]\n",
    "\n",
    "comparison_labels = [\n",
    "    \"Acute vs Control\",\n",
    "    \"Chronic vs Control\",\n",
    "    \"Chronic vs Acute\"\n",
    "]\n",
    "\n",
    "# Loop over each metric\n",
    "for metric_name, df_metric in metrics.items():\n",
    "    print(f\"\\n=== {metric_name} ===\")\n",
    "\n",
    "    # stack all session data together in one big list\n",
    "    list_all = np.hstack([df_metric['ses-1'], df_metric['ses-2'], df_metric['ses-3']])\n",
    "    stats = np.zeros((n_iterations, 3))  #place for storing t-stats for 3 comparisons\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        #now we want to randomly shuffle the values, with replacement, using resample\n",
    "        new_sample = resample(list_all) \n",
    "        #here we define the \"new\" groups - numbers are defined automatically based on the number of participants in the original groups\n",
    "        new_ses_1 = new_sample[0:n_subjects]\n",
    "        new_ses_2 = new_sample[n_subjects:2*n_subjects]\n",
    "        new_ses_3 = new_sample[2*n_subjects:3*n_subjects]\n",
    "\n",
    "        #pairwise t-tests\n",
    "        stat12, _ = ttest_rel(new_ses_1, new_ses_2)\n",
    "        stat13, _ = ttest_rel(new_ses_1, new_ses_3)\n",
    "        stat23, _ = ttest_rel(new_ses_2, new_ses_3)\n",
    "\n",
    "        stats[i, 0] = stat12\n",
    "        stats[i, 1] = stat13\n",
    "        stats[i, 2] = stat23\n",
    "\n",
    "    #real t-tests (non-permuted, without resampling)\n",
    "    stat12_real, p12_real = ttest_rel(df_metric['ses-1'], df_metric['ses-2'])\n",
    "    stat13_real, p13_real = ttest_rel(df_metric['ses-1'], df_metric['ses-3'])\n",
    "    stat23_real, p23_real = ttest_rel(df_metric['ses-2'], df_metric['ses-3'])\n",
    "\n",
    "    real_stats = [stat12_real, stat13_real, stat23_real]\n",
    "    real_pvals = [p12_real, p13_real, p23_real]\n",
    "\n",
    "    #FDR correction\n",
    "    rejected, real_pvals_fdr, _, _ = multipletests(real_pvals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "    #plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    for j, (ax, (ses_a, ses_b)) in enumerate(zip(axes, session_pairs)):\n",
    "        ax.hist(stats[:, j], bins=30, alpha=0.7)\n",
    "        ax.axvline(real_stats[j], color='red', linestyle='dashed', linewidth=2, label=f'Real t-stat\\nt = {real_stats[j]:.2f}\\np (FDR)= {real_pvals_fdr[j]:.4f}')\n",
    "        ax.set_title(f'{metric_name}\\n{comparison_labels[j]}\\n(Unpaired permutation)', fontsize=16)\n",
    "        ax.set_xlabel('t-statistic', fontsize=14)\n",
    "        ax.set_ylabel('Frequency', fontsize=14)\n",
    "\n",
    "        # ax.text(\n",
    "        #     0.85, 0.85,\n",
    "        #     f\"t = {real_stats[j]:.2f}\\n\"\n",
    "        #     f\"p = {real_pvals[j]:.4f}\\n\"\n",
    "        #     f\"FDR p = {real_pvals_fdr[j]:.4f}\",\n",
    "        #     ha='right', va='top',\n",
    "        #     transform=ax.transAxes,\n",
    "        #     fontsize=10,\n",
    "        #     bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\")\n",
    "        # )\n",
    "        \n",
    "        ax.legend(fontsize=12)  \n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #print real and FDR corrected values\n",
    "    print('Real t-statistics, raw p-values, and FDR-corrected p-values:')\n",
    "    for label, t_val, p_val, p_fdr, sig in zip(comparison_labels, real_stats, real_pvals, real_pvals_fdr, rejected):\n",
    "        significance = \"SIGNIFICANT\" if sig else \"NON-SIGNIFICANT\"\n",
    "        print(f\"{label}: t = {t_val:.3f}, p = {p_val:.4f}, FDR p = {p_fdr:.4f} [{significance}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paired permutations (pairs kept, sleep deprivation labels randomly changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### permutations for checking the robustness of LMM for global metrics - paired permutations ##############\n",
    "metrics = {\n",
    "    \"Global efficiency\": glob_eff,\n",
    "    \"Average clustering\": clustering_df,\n",
    "    \"Average path length\": path_length_df,\n",
    "    \"Modularity\": modularity_df,\n",
    "    \"Average graph distance\": avg_graph_distance_df\n",
    "}\n",
    "\n",
    "n_iterations = 10000\n",
    "session_pairs = [\n",
    "    ('ses-1', 'ses-2'),\n",
    "    ('ses-1', 'ses-3'),\n",
    "    ('ses-2', 'ses-3')\n",
    "]\n",
    "comparison_labels = [\n",
    "    \"Acute vs Control\",\n",
    "    \"Chronic vs Control\",\n",
    "    \"Chronic vs Acute\"\n",
    "]\n",
    "\n",
    "for metric_name, df_metric in metrics.items():\n",
    "    print(f\"\\n=== {metric_name} ===\")\n",
    "    n_subjects = len(df_metric)\n",
    "\n",
    "    #extract values for each session\n",
    "    ses1 = df_metric['ses-1'].values\n",
    "    ses2 = df_metric['ses-2'].values\n",
    "    ses3 = df_metric['ses-3'].values\n",
    "\n",
    "    #store session values in a dictionary\n",
    "    session_data = {\n",
    "        'ses-1': ses1,\n",
    "        'ses-2': ses2,\n",
    "        'ses-3': ses3\n",
    "    }\n",
    "\n",
    "    #prepare array for storing permutation t-stats and real stats\n",
    "    perm_stats = np.zeros((n_iterations, 3))\n",
    "    real_stats = []\n",
    "    real_pvals = []\n",
    "\n",
    "    #loop over comparisons, pairwise\n",
    "    for idx, (ses_a, ses_b) in enumerate(session_pairs):\n",
    "        x = session_data[ses_a]\n",
    "        y = session_data[ses_b]\n",
    "\n",
    "        #real t-statistic from \"session_pairs\"\n",
    "        t_real, p_real = ttest_rel(x, y)\n",
    "        real_stats.append(t_real)\n",
    "        real_pvals.append(p_real)\n",
    "\n",
    "        #permutation testing: subject-wise label flipping - random fliping of ses_a and ses_b, but within pair, 10000 times we will calculate\n",
    "        #the differences (ttest_dep) based on mixed groups (randomly flipped labels)\n",
    "        for i in range(n_iterations):\n",
    "            flip = np.random.choice([True, False], size=n_subjects) #this will generate list of \"decisions\" about flipping, lenght of this list \n",
    "            #will be equal to the number of participants (in my case 28), so each participant will be or will not be swapped\n",
    "            x_perm = np.where(flip, x, y)\n",
    "            y_perm = np.where(flip, y, x)\n",
    "            #tstat after permutations\n",
    "            t_perm, _ = ttest_rel(x_perm, y_perm)\n",
    "            perm_stats[i, idx] = t_perm\n",
    "\n",
    "    # FDR Correction\n",
    "    rejected, real_pvals_fdr, _, _ = multipletests(real_pvals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "    #plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    for j, ax in enumerate(axes):\n",
    "        ax.hist(perm_stats[:, j], bins=30, alpha=0.7)\n",
    "        ax.axvline(real_stats[j], color='red', linestyle='dashed', linewidth=2, label=f'Real t-stat\\nt = {real_stats[j]:.2f}\\np (FDR)= {real_pvals_fdr[j]:.4f}')\n",
    "        ax.set_title(f'{metric_name}\\n{comparison_labels[j]}\\n(Paired permutation)', fontsize=16)\n",
    "        ax.set_xlabel('t-statistic', fontsize=14)\n",
    "        ax.set_ylabel('Frequency', fontsize=14)\n",
    "\n",
    "        # ax.text(\n",
    "        #     0.85, 0.85,\n",
    "        #     f\"t = {real_stats[j]:.2f}\\n\"\n",
    "        #     f\"p = {real_pvals[j]:.4f}\\n\"\n",
    "        #     f\"FDR p = {real_pvals_fdr[j]:.4f}\",\n",
    "        #     ha='right', va='top',\n",
    "        #     transform=ax.transAxes,\n",
    "        #     fontsize=10,\n",
    "        #     bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\")\n",
    "        # )\n",
    "\n",
    "        ax.legend(fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #print real and FDR corrected values\n",
    "    print('Real t-statistics, raw p-values, and FDR-corrected p-values:')\n",
    "    for label, t_val, p_val, p_fdr, sig in zip(comparison_labels, real_stats, real_pvals, real_pvals_fdr, rejected):\n",
    "        significance = \"SIGNIFICANT\" if sig else \"NON-SIGNIFICANT\"\n",
    "        print(f\"{label}: t = {t_val:.3f}, p = {p_val:.4f}, FDR p = {p_fdr:.4f} [{significance}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nodal metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "\n",
    "session_labels = {\n",
    "    \"ses-1\": \"baseline\",\n",
    "    \"ses-2\": \"acute\",\n",
    "    \"ses-3\": \"chronic\"\n",
    "}\n",
    "\n",
    "metrics_list = [\"Closeness\", \"Betweenness\", \"Clustering\", \"Degree\"]\n",
    "n_regions = 89\n",
    "\n",
    "output_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/nodal_metrics_LMM\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "lmm_results = []\n",
    "\n",
    "def run_lmm_with_ref(df_long, metric_name, ref_session, region_index):\n",
    "    df_long[\"session\"] = pd.Categorical(df_long[\"session\"], categories=[\"ses-1\", \"ses-2\", \"ses-3\"])\n",
    "    df_long[\"session\"] = df_long[\"session\"].cat.reorder_categories([ref_session,\n",
    "                                                                    *(s for s in [\"ses-1\", \"ses-2\", \"ses-3\"] if s != ref_session)])\n",
    "    try:\n",
    "        model = mixedlm(\"value ~ session\", data=df_long, groups=df_long[\"subject_id\"])\n",
    "        result = model.fit()\n",
    "\n",
    "        for param in result.fe_params.index:\n",
    "            if param == \"Intercept\":\n",
    "                continue\n",
    "            pval = result.pvalues[param]\n",
    "            estimate = result.fe_params[param]\n",
    "            lmm_results.append({\n",
    "                \"Region\": region_index,\n",
    "                \"Metric\": metric_name,\n",
    "                \"Reference Session\": ref_session,\n",
    "                \"Effect\": param,\n",
    "                \"Estimate\": estimate,\n",
    "                \"p-value\": pval\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model for {metric_name} with ref {ref_session}, region {region_index}: {e}\")\n",
    "\n",
    "#loop over metrics\n",
    "for metric in metrics_list:\n",
    "    print(f\"\\nProcessing metric: {metric}\")\n",
    "    pvals_per_region = np.full((n_regions, 3), np.nan)\n",
    "    estimates_per_region = np.full((n_regions, 3), np.nan)\n",
    "\n",
    "    for region in range(n_regions):\n",
    "        data_long = []\n",
    "\n",
    "        for ses_id in session_labels.keys():\n",
    "            ses_path = os.path.join(base_path, ses_id)\n",
    "            subject_paths = glob.glob(os.path.join(ses_path, \"sub-*\"))\n",
    "\n",
    "            for sub_path in subject_paths:\n",
    "                sub_id = os.path.basename(sub_path)\n",
    "                graph_path = os.path.join(sub_path, \"Graphs/wAALours\", \"graph_metrics\")\n",
    "                metric_files = glob.glob(os.path.join(graph_path, \"*_metrics.csv\"))\n",
    "\n",
    "                for metrics_file in metric_files:\n",
    "                    try:\n",
    "                        df_metrics = pd.read_csv(metrics_file)\n",
    "                        value = df_metrics.loc[df_metrics['Node'] == region, metric].values[0]\n",
    "                        data_long.append({\n",
    "                            'subject_id': sub_id,\n",
    "                            'session': ses_id,\n",
    "                            'value': value\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {metrics_file}: {e}\")\n",
    "                        continue\n",
    "\n",
    "        df_long = pd.DataFrame(data_long)\n",
    "\n",
    "        if df_long['session'].nunique() < 2 or df_long['subject_id'].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        #reference sessions 1 and 2!!!\n",
    "        run_lmm_with_ref(df_long.copy(), metric, ref_session=\"ses-1\", region_index=region)\n",
    "        run_lmm_with_ref(df_long.copy(), metric, ref_session=\"ses-2\", region_index=region)\n",
    "\n",
    "    df_lmm = pd.DataFrame(lmm_results)\n",
    "\n",
    "    for i in range(n_regions):\n",
    "        # p-values and estimates\n",
    "        for j, (ref, contrast, col) in enumerate([\n",
    "            (\"ses-1\", \"session[T.ses-2]\", 0),\n",
    "            (\"ses-1\", \"session[T.ses-3]\", 1),\n",
    "            (\"ses-2\", \"session[T.ses-3]\", 2)\n",
    "        ]):\n",
    "            match = df_lmm[(df_lmm[\"Metric\"] == metric) & (df_lmm[\"Region\"] == i) &\n",
    "                           (df_lmm[\"Reference Session\"] == ref) &\n",
    "                           (df_lmm[\"Effect\"] == contrast)]\n",
    "            if not match.empty:\n",
    "                pvals_per_region[i, col] = match[\"p-value\"].values[0]\n",
    "                estimates_per_region[i, col] = match[\"Estimate\"].values[0]\n",
    "\n",
    "    #save both results\n",
    "    df_pvals = pd.DataFrame(pvals_per_region, columns=[\"ses1_vs_ses2\", \"ses1_vs_ses3\", \"ses2_vs_ses3\"])\n",
    "    df_ests = pd.DataFrame(estimates_per_region, columns=[\"ses1_vs_ses2\", \"ses1_vs_ses3\", \"ses2_vs_ses3\"])\n",
    "\n",
    "    df_pvals.to_csv(os.path.join(output_dir, f\"{metric.lower()}_pvals_per_region_LMM.csv\"), index=False)\n",
    "    df_ests.to_csv(os.path.join(output_dir, f\"{metric.lower()}_estimates_per_region_LMM.csv\"), index=False)\n",
    "\n",
    "    print(f\"Saved: {metric.lower()}_pvals_per_region_LMM.csv\")\n",
    "    print(f\"Saved: {metric.lower()}_estimates_per_region_LMM.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder where the per-region p-value CSVs are saved\n",
    "output_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics\"\n",
    "\n",
    "#list of metrics \n",
    "metrics_list = [\"closeness\", \"betweenness\", \"clustering\", \"degree\"]\n",
    "\n",
    "#load all p-values into a dictionary\n",
    "pvals_all = {}\n",
    "\n",
    "for metric in metrics_list:\n",
    "    file_path = os.path.join(output_dir, f\"{metric}_pvals_per_region_LMM.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        pvals_all[metric.capitalize()] = df.values  \n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histograms\n",
    "session_pairs = ['Ses-1 (control) vs Ses-2 (acute)', 'Ses-1 (control) vs Ses-3 (chronic)', 'Ses-2 (acute) vs Ses-3 (chronic)']\n",
    "\n",
    "#loop over metrics, pvals_all is the dictionary from the previous code\n",
    "for metric_name, pvals in pvals_all.items():\n",
    "    print(f\"\\nPlotting for {metric_name}\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.hist(pvals[:, i], bins=20)\n",
    "        ax.set_title(f'{metric_name} - {session_pairs[i]}')\n",
    "        ax.set_xlabel('p-value')\n",
    "        ax.set_ylabel('Number of regions')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple comparisons correction (nodal level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_results = {}\n",
    "\n",
    "session_comparisons = ['ses1_vs_ses2', 'ses1_vs_ses3', 'ses2_vs_ses3']\n",
    "\n",
    "for metric_name, pvals in pvals_all.items():\n",
    "    print(f\"\\n=== Multiple comparisons correction for {metric_name} ===\")\n",
    "\n",
    "    correction_results[metric_name] = {}\n",
    "\n",
    "    for i_session, session_comparison  in enumerate(session_comparisons):\n",
    "        print(f\"\\n--- {session_comparison } ---\")\n",
    "\n",
    "        pvals_session = pvals[:, i_session]  # p-values for this sessions comparison\n",
    "\n",
    "        #bonferroni correction\n",
    "        rej_bonf = bonferroni(pvals_session, alpha=0.05)\n",
    "        print('Bonferroni reject decisions:')\n",
    "        print(rej_bonf)\n",
    "\n",
    "        # FDR correction (Benjamini-Hochberg LSU)\n",
    "        rej_fdr = lsu(pvals_session, q=0.1)\n",
    "        print('FDR (LSU) reject decisions:')\n",
    "        print(rej_fdr)\n",
    "\n",
    "        #save into dictionary\n",
    "        correction_results[metric_name][session_comparison] = {\n",
    "            \"bonferroni\": rej_bonf,\n",
    "            \"fdr\": rej_fdr\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## plot significant ROIs of the graph on the brain #####\n",
    "\n",
    "os.makedirs(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/significant_nodes\", exist_ok=True)\n",
    "\n",
    "#ROI names\n",
    "roi_table_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/AAL_ours_89_regions_list.csv\"\n",
    "roi_df = pd.read_csv(roi_table_path, sep=\";\")\n",
    "roi_df.columns = roi_df.columns.str.strip()\n",
    "roi_lookup = dict(zip(roi_df[\"Node_number\"], roi_df[\"Region\"]))\n",
    "\n",
    "#coordinates and adjacency matrix \n",
    "coord_path = \"/Users/patrycjascislewska/Analizy_neuro/AAL3/AALours_coords_MNI.txt\"\n",
    "coord = pd.read_csv(coord_path, sep=\"\\s+\", header=None).to_numpy()\n",
    "\n",
    "adj_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/ses-1/sub-14/Graphs/wAALours/Adj_mat_n.levels_3_n.regions_89proc_length384.cost_400.txt\"\n",
    "adj_matrix = pd.read_csv(adj_path, sep=\"\\s+\", header=None).to_numpy()\n",
    "G = nx.from_numpy_array(adj_matrix)\n",
    "\n",
    "#metrics and session comparisons\n",
    "metrics_list = [\"Closeness\", \"Betweenness\", \"Clustering\", \"Degree\"]\n",
    "session_comparisons = ['ses1_vs_ses2', 'ses1_vs_ses3', 'ses2_vs_ses3']\n",
    "\n",
    "#comparison labels\n",
    "comparison_labels_map = {\n",
    "    'ses1_vs_ses2': 'Acute vs Control',\n",
    "    'ses1_vs_ses3': 'Chronic vs Control',\n",
    "    'ses2_vs_ses3': 'Chronic vs Acute'\n",
    "}\n",
    "\n",
    "#file mapping for LMM coefficient paths\n",
    "coef_file_map = {\n",
    "    \"Degree\": \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/nodal_metrics_LMM/degree_estimates_per_region_LMM.csv\",\n",
    "    \"Clustering\": \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/nodal_metrics_LMM/clustering_estimates_per_region_LMM.csv\",\n",
    "    \"Closeness\": \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/nodal_metrics_LMM/closeness_estimates_per_region_LMM.csv\",\n",
    "    \"Betweenness\": \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/nodal_metrics_LMM/betweenness_estimates_per_region_LMM.csv\"\n",
    "}\n",
    "\n",
    "#main plotting loop \n",
    "for metric_name in metrics_list:\n",
    "    #load coefficients\n",
    "    coef_path = coef_file_map[metric_name]\n",
    "    coef_df = pd.read_csv(coef_path, sep=\",\")\n",
    "    coef_df.columns = coef_df.columns.str.strip()\n",
    "\n",
    "    for session_comparison in session_comparisons:\n",
    "        print(f\"\\n=== {metric_name} - {session_comparison} ===\")\n",
    "\n",
    "    \n",
    "        fdr_significant = correction_results[metric_name][session_comparison][\"fdr\"].astype(bool)\n",
    "        coef_session = coef_df[session_comparison].to_numpy()\n",
    "\n",
    "        #encode colors: -1 = â†“, 1 = â†‘, 0 = n.s.\n",
    "        node_colors = np.zeros_like(coef_session)\n",
    "        node_colors[fdr_significant & (coef_session > 0)] = 1\n",
    "        node_colors[fdr_significant & (coef_session < 0)] = -1\n",
    "\n",
    "        #significant nodes with region names and coordinates\n",
    "        print(\"Significant nodes (FDR-corrected):\")\n",
    "        significant_nodes_data = []\n",
    "\n",
    "        for idx, (is_sig, coef) in enumerate(zip(fdr_significant, coef_session)):\n",
    "            if is_sig:\n",
    "                direction = \"â†‘\" if coef > 0 else \"â†“\"\n",
    "                roi_name = roi_lookup.get(idx, f\"ROI_{idx}\")\n",
    "                coord_str = f\"({coord[idx,0]:.1f}, {coord[idx,1]:.1f}, {coord[idx,2]:.1f})\"\n",
    "                print(f\"Node {idx:02d} ({roi_name}): {direction} coef = {coef:.4f}  |  Coord: {coord_str}\")\n",
    "                significant_nodes_data.append({\n",
    "                    \"Node\": idx,\n",
    "                    \"Region\": roi_name,\n",
    "                    \"X\": round(coord[idx, 0], 1),\n",
    "                    \"Y\": round(coord[idx, 1], 1),\n",
    "                    \"Z\": round(coord[idx, 2], 1),\n",
    "                    \"Coefficient\": round(coef, 4),\n",
    "                    \"Direction\": \"increase\" if coef > 0 else \"decrease\"\n",
    "                })\n",
    "\n",
    "        #save significant nodes to CSV\n",
    "        if significant_nodes_data:\n",
    "            sig_df = pd.DataFrame(significant_nodes_data)\n",
    "            out_path = f\"/Users/patrycjascislewska/Analizy_neuro/Graphs/significant_nodes/{metric_name}_{session_comparison}_sig_nodes.csv\"\n",
    "            sig_df.to_csv(out_path, index=False)\n",
    "            print(f\"\\nSaved significant node data to:\\n{out_path}\")\n",
    "\n",
    "        #plotting setup\n",
    "        color_map = { -1: '#0080FF', 0: '#A0A0A0', 1: '#FF8000' }\n",
    "        node_color_list = [color_map[val] for val in node_colors]\n",
    "        edge_color = '0.8'\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "        # Sagittal view (X-Z)\n",
    "        pos_sagittal = {k: (coord[k, 1], coord[k, 2]) for k in range(89)}\n",
    "        nx.draw(G, pos_sagittal, node_color=node_color_list, edge_color=edge_color,\n",
    "                with_labels=False, node_size=100, ax=axs[0])\n",
    "        axs[0].set_title('Sagittal view (Y-Z)')\n",
    "\n",
    "        # Axial view (X-Y)\n",
    "        pos_axial = {k: (coord[k, 1], coord[k, 0]) for k in range(89)}\n",
    "        nx.draw(G, pos_axial, node_color=node_color_list, edge_color=edge_color,\n",
    "                with_labels=False, node_size=100, ax=axs[1])\n",
    "        axs[1].set_title('Axial view (X-Y)')\n",
    "\n",
    "        # Coronal view (Y-Z)\n",
    "        pos_coronal = {k: (coord[k, 0], coord[k, 2]) for k in range(89)}\n",
    "        nx.draw(G, pos_coronal, node_color=node_color_list, edge_color=edge_color,\n",
    "                with_labels=False, node_size=100, ax=axs[2])\n",
    "        axs[2].set_title('Coronal view (X-Z)')\n",
    "\n",
    "        # Legend\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='w', label='Significant increase',\n",
    "                   markerfacecolor='#FF8000', markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label='Significant decrease',\n",
    "                   markerfacecolor='#0080FF', markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label='Not significant',\n",
    "                   markerfacecolor='#A0A0A0', markersize=10)\n",
    "        ]\n",
    "        axs[2].legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "        fig.suptitle(\n",
    "            f\"{metric_name} - {comparison_labels_map.get(session_comparison, session_comparison)}\",\n",
    "            fontsize=16\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodal results on the glass brain template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plotting significant nodes (only) on the glass brain using netplotbrain package\n",
    "\n",
    "from netplotbrain import plot\n",
    "\n",
    "sig_nodes_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/significant_nodes\"\n",
    "output_dir = os.path.join(sig_nodes_dir, \"brain_plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# all significant node CSVs \n",
    "sig_files = glob.glob(os.path.join(sig_nodes_dir, \"*_sig_nodes.csv\"))\n",
    "\n",
    "# color mapping for directions\n",
    "direction_color_map = {\n",
    "    \"increase\": \"#FF8000\",  # orange\n",
    "    \"decrease\": \"#0080FF\"   # blue\n",
    "}\n",
    "\n",
    "#loop through all \"siginificant_nodes\" files and and plot\n",
    "for file in sig_files:\n",
    "    df = pd.read_csv(file)\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    #extract metric and comparison from filename\n",
    "    basename = os.path.basename(file)\n",
    "    parts = basename.replace(\"_sig_nodes.csv\", \"\").split(\"_\")\n",
    "    metric = parts[0]\n",
    "    comparison = \"_\".join(parts[1:])\n",
    "\n",
    "    #prepare plotting df to fit the netplotbrain requirements\n",
    "    df_plot = df.copy()\n",
    "    df_plot[\"x\"] = df_plot[\"X\"]\n",
    "    df_plot[\"y\"] = df_plot[\"Y\"]\n",
    "    df_plot[\"z\"] = df_plot[\"Z\"]\n",
    "    df_plot[\"label\"] = df_plot[\"Region\"]\n",
    "    df_plot[\"color\"] = df_plot[\"Direction\"].map(direction_color_map)\n",
    "\n",
    "    fig, ax = plot(\n",
    "        nodes=df_plot,\n",
    "        node_color=\"color\",\n",
    "        node_size=20,\n",
    "        node_labels=False,\n",
    "        view=\"APLRISs\", # here we can change views\n",
    "        template='MNI152NLin6Asym',\n",
    "        template_style='glass', # there is also e.g. surface \n",
    "        title=f\"{metric} - {comparison}\"\n",
    "    )\n",
    "\n",
    "    #save plot\n",
    "    output_path = os.path.join(output_dir, f\"{metric}_{comparison}_brain.png\")\n",
    "    fig.savefig(output_path, dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-Subject Hub Disruption Index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I use the degree from previously saved files with nodal metrics\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"acute\": os.path.join(base_path, \"ses-2\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs in all 3 sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses2 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"acute\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses2 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects present in all 3 sessions: {len(common_ids)}\")\n",
    "\n",
    "#load degree vectors from metrics CSV (previously saved)\n",
    "def load_degree_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Degree_centrality\"].values\n",
    "\n",
    "#plot multiple histograms - one per subject per comparison\n",
    "fig, axes = plt.subplots(4, len(common_ids), figsize=(4 * len(common_ids), 12), sharey=True)\n",
    "\n",
    "#kappas storage\n",
    "kappa_acute_vs_control = []\n",
    "kappa_chronic_vs_control = []\n",
    "kappa_chronic_vs_acute = []\n",
    "kappa_acute_vs_chronic = []\n",
    "\n",
    "#main loop\n",
    "for i, subject_id in enumerate(common_ids):\n",
    "    deg_control = load_degree_vector(subject_id, \"control\")\n",
    "    deg_acute = load_degree_vector(subject_id, \"acute\")\n",
    "    deg_chronic = load_degree_vector(subject_id, \"chronic\")\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Acute vs control\n",
    "    x = deg_control.reshape((-1, 1))\n",
    "    y = deg_acute - deg_control \n",
    "    model.fit(x, y)\n",
    "    kappa_acute_vs_control.append(model.coef_[0])\n",
    "    axes[0, i].scatter(deg_control, y, alpha=0.7)\n",
    "    axes[0, i].plot(deg_control, model.predict(x), color='red')\n",
    "    axes[0, i].set_title(f\"{subject_id}\\nÎº1={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel(\"Î” Degree (Acute - Control)\")\n",
    "\n",
    "    # Chronic vs control\n",
    "    y = deg_chronic - deg_control \n",
    "    model.fit(x, y)\n",
    "    kappa_chronic_vs_control.append(model.coef_[0])\n",
    "    axes[1, i].scatter(deg_control, y, alpha=0.7)\n",
    "    axes[1, i].plot(deg_control, model.predict(x), color='red')\n",
    "    axes[1, i].set_title(f\"Îº2={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Î” Degree (Chronic - Control)\")\n",
    "\n",
    "    # Chronic vs Acute\n",
    "    x = deg_acute.reshape((-1, 1))\n",
    "    y = deg_chronic - deg_acute \n",
    "    model.fit(x, y)\n",
    "    kappa_chronic_vs_acute.append(model.coef_[0])\n",
    "    axes[2, i].scatter(deg_acute, y, alpha=0.7)\n",
    "    axes[2, i].plot(deg_acute, model.predict(x), color='red')\n",
    "    axes[2, i].set_title(f\"Îº3={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel(\"Î” Degree (Chronic - Acute)\")\n",
    "\n",
    "    # Acute vs Chronic\n",
    "    x = deg_chronic.reshape((-1, 1))\n",
    "    y = deg_acute - deg_chronic \n",
    "    model.fit(x, y)\n",
    "    kappa_acute_vs_chronic.append(model.coef_[0])\n",
    "    axes[3, i].scatter(deg_chronic, y, alpha=0.7)\n",
    "    axes[3, i].plot(deg_chronic, model.predict(x), color='red')\n",
    "    axes[3, i].set_title(f\"Îº4={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel(\"Î” Degree (Acute - Chronic)\")\n",
    "\n",
    "#plot\n",
    "plt.suptitle(\"Within subject 'HDI' scatterplots using degree\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/HDI_kappa_results_within_subject\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_path = os.path.join(output_dir, \"kappas_degree.csv\")\n",
    "# df_kappa.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "closenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I use the closeness values from previously saved files with nodal metrics\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"acute\": os.path.join(base_path, \"ses-2\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs in all 3 sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses2 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"acute\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses2 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects present in all 3 sessions: {len(common_ids)}\")\n",
    "\n",
    "#load closeness vectors from metrics CSV (previously saved)\n",
    "def load_Closeness_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Closeness\"].values\n",
    "\n",
    "#plot multiple histograms - one per subject per comparison\n",
    "fig, axes = plt.subplots(4, len(common_ids), figsize=(4 * len(common_ids), 12), sharey=True)\n",
    "\n",
    "#kappas storage\n",
    "kappa_acute_vs_control = []\n",
    "kappa_chronic_vs_control = []\n",
    "kappa_chronic_vs_acute = []\n",
    "kappa_acute_vs_chronic = []\n",
    "\n",
    "#main loop\n",
    "for i, subject_id in enumerate(common_ids):\n",
    "    Closeness_control = load_Closeness_vector(subject_id, \"control\")\n",
    "    Closeness_acute = load_Closeness_vector(subject_id, \"acute\")\n",
    "    Closeness_chronic = load_Closeness_vector(subject_id, \"chronic\")\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Acute vs control\n",
    "    x = Closeness_control.reshape((-1, 1))\n",
    "    y = Closeness_acute - Closeness_control \n",
    "    kappa_acute_vs_control.append(model.coef_[0])\n",
    "    axes[0, i].scatter(Closeness_control, y, alpha=0.7)\n",
    "    axes[0, i].plot(Closeness_control, model.predict(x), color='red')\n",
    "    axes[0, i].set_title(f\"{subject_id}\\nÎº1={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel(\"Î” Closeness (Acute - Control)\")\n",
    "\n",
    "    # Chronic vs control\n",
    "    y = Closeness_chronic - Closeness_control \n",
    "    model.fit(x, y)\n",
    "    kappa_chronic_vs_control.append(model.coef_[0])\n",
    "    axes[1, i].scatter(Closeness_control, y, alpha=0.7)\n",
    "    axes[1, i].plot(Closeness_control, model.predict(x), color='red')\n",
    "    axes[1, i].set_title(f\"Îº2={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Î” Closeness (Chronic - Control)\")\n",
    "\n",
    "    # Chronic vs Acute\n",
    "    x = Closeness_acute.reshape((-1, 1))\n",
    "    y = Closeness_chronic - Closeness_acute \n",
    "    model.fit(x, y)\n",
    "    kappa_chronic_vs_acute.append(model.coef_[0])\n",
    "    axes[2, i].scatter(Closeness_acute, y, alpha=0.7)\n",
    "    axes[2, i].plot(Closeness_acute, model.predict(x), color='red')\n",
    "    axes[2, i].set_title(f\"Îº3={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel(\"Î” Closeness (Chronic - Acute)\")\n",
    "\n",
    "    # Acute vs Chronic\n",
    "    x = Closeness_chronic.reshape((-1, 1))\n",
    "    y = Closeness_acute - Closeness_chronic \n",
    "    model.fit(x, y)\n",
    "    kappa_acute_vs_chronic.append(model.coef_[0])\n",
    "    axes[3, i].scatter(Closeness_chronic, y, alpha=0.7)\n",
    "    axes[3, i].plot(Closeness_chronic, model.predict(x), color='red')\n",
    "    axes[3, i].set_title(f\"Îº4={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel(\"Î” Closeness (Acute - Chronic)\")\n",
    "\n",
    "#plot\n",
    "plt.suptitle(\"Within subject 'HDI' scatterplots using Closeness\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/HDI_kappa_results_within_subject\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_path = os.path.join(output_dir, \"kappas_closeness.csv\")\n",
    "# df_kappa.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I use the clustering coefficients from previously saved files with nodal metrics\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"acute\": os.path.join(base_path, \"ses-2\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs in all 3 sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses2 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"acute\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses2 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects present in all 3 sessions: {len(common_ids)}\")\n",
    "\n",
    "#load clustering vectors from metrics CSV (previously saved)\n",
    "def load_Clustering_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Clustering\"].values\n",
    "\n",
    "#plot multiple histograms - one per subject per comparison\n",
    "fig, axes = plt.subplots(4, len(common_ids), figsize=(4 * len(common_ids), 12), sharey=True)\n",
    "\n",
    "#kappas storage\n",
    "kappa_acute_vs_control = []\n",
    "kappa_chronic_vs_control = []\n",
    "kappa_chronic_vs_acute = []\n",
    "kappa_acute_vs_chronic = []\n",
    "\n",
    "#main loop\n",
    "for i, subject_id in enumerate(common_ids):\n",
    "    Clustering_control = load_Clustering_vector(subject_id, \"control\")\n",
    "    Clustering_acute = load_Clustering_vector(subject_id, \"acute\")\n",
    "    Clustering_chronic = load_Clustering_vector(subject_id, \"chronic\")\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Acute vs control\n",
    "    x = Clustering_control.reshape((-1, 1))\n",
    "    y = Clustering_acute - Clustering_control \n",
    "    model.fit(x, y)\n",
    "    kappa_acute_vs_control.append(model.coef_[0])\n",
    "    axes[0, i].scatter(Clustering_control, y, alpha=0.7)\n",
    "    axes[0, i].plot(Clustering_control, model.predict(x), color='red')\n",
    "    axes[0, i].set_title(f\"{subject_id}\\nÎº1={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel(\"Î” Clustering (Acute - Control)\")\n",
    "\n",
    "    # Chronic vs control\n",
    "    y = Clustering_chronic - Clustering_control \n",
    "    model.fit(x, y)\n",
    "    kappa_chronic_vs_control.append(model.coef_[0])\n",
    "    axes[1, i].scatter(Clustering_control, y, alpha=0.7)\n",
    "    axes[1, i].plot(Clustering_control, model.predict(x), color='red')\n",
    "    axes[1, i].set_title(f\"Îº2={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Î” Clustering (Chronic - Control)\")\n",
    "\n",
    "    # Chronic vs Acute\n",
    "    x = Clustering_acute.reshape((-1, 1))\n",
    "    y = Clustering_chronic - Clustering_acute\n",
    "    model.fit(x, y)\n",
    "    kappa_chronic_vs_acute.append(model.coef_[0])\n",
    "    axes[2, i].scatter(Clustering_acute, y, alpha=0.7)\n",
    "    axes[2, i].plot(Clustering_acute, model.predict(x), color='red')\n",
    "    axes[2, i].set_title(f\"Îº3={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel(\"Î” Clustering (Chronic - Acute)\")\n",
    "\n",
    "    # Acute vs Chronic\n",
    "    x = Clustering_chronic.reshape((-1, 1))\n",
    "    y = Clustering_acute - Clustering_chronic \n",
    "    model.fit(x, y)\n",
    "    kappa_acute_vs_chronic.append(model.coef_[0])\n",
    "    axes[3, i].scatter(Clustering_chronic, y, alpha=0.7)\n",
    "    axes[3, i].plot(Clustering_chronic, model.predict(x), color='red')\n",
    "    axes[3, i].set_title(f\"Îº4={model.coef_[0]:.3f}\")\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel(\"Î” Clustering (Acute - Chronic)\")\n",
    "\n",
    "#plot\n",
    "plt.suptitle(\"Within subject 'HDI' scatterplots using Clustering\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/HDI_kappa_results_within_subject\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_path = os.path.join(output_dir, \"kappas_clustering.csv\")\n",
    "# df_kappa.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of within-subject Hub Disruption Index (permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kappa within-subject  -  kappa group ###\n",
    "\n",
    "\n",
    "#real degree vectors from our csv files\n",
    "def load_degree_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Degree_centrality\"].values\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"acute\": os.path.join(base_path, \"ses-2\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses2 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"acute\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses2))\n",
    "\n",
    "print(f\"Subjects in both control and acute: {len(common_ids)}\")\n",
    "\n",
    "#load degree vectors\n",
    "deg_control_all = []\n",
    "deg_acute_all = []\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    deg_control = load_degree_vector(subject_id, \"control\")\n",
    "    deg_acute = load_degree_vector(subject_id, \"acute\")\n",
    "    deg_control_all.append(deg_control)\n",
    "    deg_acute_all.append(deg_acute)\n",
    "\n",
    "deg_control_all = np.array(deg_control_all)\n",
    "deg_acute_all = np.array(deg_acute_all)\n",
    "\n",
    "#compute group-averaged control degree vector\n",
    "group_control_mean = np.mean(deg_control_all, axis=0)\n",
    "\n",
    "#compute real kappa differences (within kappa - group average kappa)\n",
    "real_diffs = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(len(common_ids)):\n",
    "    subj_control = deg_control_all[i]\n",
    "    subj_acute = deg_acute_all[i]\n",
    "    \n",
    "    # 1. Within-subject\n",
    "    delta_within = subj_acute - subj_control\n",
    "    model.fit(subj_control.reshape(-1, 1), delta_within)\n",
    "    kappa_within = model.coef_[0]\n",
    "    \n",
    "    # 2. Group-based\n",
    "    delta_groupmean = subj_acute - group_control_mean\n",
    "    model.fit(group_control_mean.reshape(-1, 1), delta_groupmean)\n",
    "    kappa_groupmean = model.coef_[0]\n",
    "    \n",
    "    real_diffs.append(kappa_within - kappa_groupmean)\n",
    "\n",
    "real_diffs = np.array(real_diffs)\n",
    "real_mean_diff = np.mean(real_diffs)\n",
    "print(f\"Real mean (Îº_within - Îº_groupmean): {real_mean_diff:.4f}\")\n",
    "\n",
    "# PERMUTATIONS - Shuffle node order in chronic session\n",
    "n_iterations = 10000\n",
    "null_diffs = np.zeros(n_iterations)\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    diffs = []\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        subj_control = deg_control_all[i]\n",
    "        subj_acute = np.random.permutation(deg_acute_all[i])  # shuffle node order\n",
    "        \n",
    "        # 1. Within-subject\n",
    "        delta_within = subj_acute - subj_control\n",
    "        model.fit(subj_control.reshape(-1, 1), delta_within)\n",
    "        kappa_within = model.coef_[0]\n",
    "        \n",
    "        # 2. Group-based\n",
    "        delta_groupmean = subj_acute - group_control_mean\n",
    "        model.fit(group_control_mean.reshape(-1, 1), delta_groupmean)\n",
    "        kappa_groupmean = model.coef_[0]\n",
    "        \n",
    "        diffs.append(kappa_within - kappa_groupmean)\n",
    "    \n",
    "    null_diffs[it] = np.mean(diffs)\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(null_diffs, bins=30, alpha=0.7)\n",
    "plt.axvline(real_mean_diff, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f\"Real mean diff = {real_mean_diff:.3f}\")\n",
    "plt.xlabel(\"Mean Îº_within âˆ’ Îº_groupmean (permutation distribution)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Permutations distribution of 'kappa within - kappa group' after node shuffling \\n(acute nodes shuffled, control-acute pairs kept in order)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# P-value\n",
    "p_value = np.mean(null_diffs <= real_mean_diff)\n",
    "print(f\"P-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the \"mixed WITHIN PAIRS\" aproach ## \n",
    "\n",
    "#real degree vectors from our csv files\n",
    "def load_degree_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Degree_centrality\"].values\n",
    "\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects in both control and chronic: {len(common_ids)}\")\n",
    "\n",
    "#load degree vectors\n",
    "deg_control_all = []\n",
    "deg_chronic_all = []\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    deg_control = load_degree_vector(subject_id, \"control\")\n",
    "    deg_chronic = load_degree_vector(subject_id, \"chronic\")\n",
    "    deg_control_all.append(deg_control)\n",
    "    deg_chronic_all.append(deg_chronic)\n",
    "\n",
    "deg_control_all = np.array(deg_control_all)\n",
    "deg_chronic_all = np.array(deg_chronic_all)\n",
    "\n",
    "#COMPUTE REAL KAPPAS\n",
    "real_kappas = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(len(common_ids)):\n",
    "    x = deg_control_all[i].reshape(-1, 1)  \n",
    "    y = deg_chronic_all[i] - deg_control_all[i]\n",
    "    model.fit(x, y)\n",
    "    real_kappas.append(model.coef_[0])\n",
    "\n",
    "real_kappas = np.array(real_kappas)\n",
    "real_kappa_mean = np.mean(real_kappas)\n",
    "print(f\"Real mean kappa (chronic vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    "\n",
    "n_iterations = 10000\n",
    "null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    perm_kappas = []\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = deg_control_all[i].reshape(-1, 1)\n",
    "        delta = deg_chronic_all[i] - deg_control_all[i] \n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            delta = -delta\n",
    "\n",
    "        model.fit(x, delta)\n",
    "        perm_kappas.append(model.coef_[0])\n",
    "\n",
    "    null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "#plot \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(null_kappa_means, bins=30, alpha=0.7)\n",
    "plt.axvline(real_kappa_mean, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f\"Real Îº = {real_kappa_mean:.3f}\")\n",
    "plt.xlabel(\"Mean kappa (permutation distribution)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Permutations - distribution of mean kappa (chronic vs control) \\n (flip signs of Î”y ONLY and keep x axis the same)\\n mixed WITHIN pairs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the \"mixed WITHIN PAIRS\" aproach ##\n",
    "\n",
    "#real vectors from our csv files\n",
    "def load_metric_vector(subject_id, session, metric_name):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[metric_name].values\n",
    "\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"acute\": os.path.join(base_path, \"ses-2\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses2 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"acute\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses2))\n",
    "\n",
    "print(f\"Subjects in both control and acute: {len(common_ids)}\")\n",
    "\n",
    "metric_names = [\"Degree_centrality\", \"Closeness\", \"Clustering\"]\n",
    "\n",
    "real_kappa_means = {}\n",
    "null_kappa_distributions = {}\n",
    "pvals_uncorrected = {}\n",
    "cohen_ds = {}\n",
    "\n",
    "# loop over each metric\n",
    "for metric_name in metric_names:\n",
    "    #load metric vectors\n",
    "    control_all = []\n",
    "    acute_all = []\n",
    "\n",
    "    for subject_id in common_ids:\n",
    "        control = load_metric_vector(subject_id, \"control\", metric_name)\n",
    "        acute = load_metric_vector(subject_id, \"acute\", metric_name)\n",
    "        control_all.append(control)\n",
    "        acute_all.append(acute)\n",
    "\n",
    "    control_all = np.array(control_all)\n",
    "    acute_all = np.array(acute_all)\n",
    "\n",
    "    #COMPUTE REAL KAPPAS\n",
    "    real_kappas = []\n",
    "    model = LinearRegression()\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = control_all[i].reshape(-1, 1) \n",
    "        y = acute_all[i] - control_all[i]\n",
    "        model.fit(x, y)\n",
    "        real_kappas.append(model.coef_[0])\n",
    "\n",
    "    real_kappas = np.array(real_kappas)\n",
    "    real_kappa_mean = np.mean(real_kappas)\n",
    "    real_kappa_means[metric_name] = real_kappa_mean\n",
    "    print(f\"{metric_name} - Real mean kappa (acute vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    " \n",
    "    n_iterations = 10000\n",
    "    null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "    for it in range(n_iterations):\n",
    "        perm_kappas = []\n",
    "\n",
    "        for i in range(len(common_ids)):\n",
    "            x = control_all[i].reshape(-1, 1) \n",
    "            delta = acute_all[i] - control_all[i] \n",
    "\n",
    "\n",
    "            if np.random.rand() < 0.5:\n",
    "                delta = -delta\n",
    "\n",
    "            model.fit(x, delta)\n",
    "            perm_kappas.append(model.coef_[0])\n",
    "\n",
    "        null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "    null_kappa_distributions[metric_name] = null_kappa_means\n",
    "\n",
    "    #p-value (two-tailed), this counts how many null values are as extreme or more extreme than the absolute value of my observed effect.\n",
    "    p_value_two_tailed = (np.sum(np.abs(null_kappa_means) >= np.abs(real_kappa_mean)) + 1) / (n_iterations + 1) \n",
    "    pvals_uncorrected[metric_name] = p_value_two_tailed\n",
    "\n",
    "    #Cohen's d\n",
    "    mean_null = np.mean(null_kappa_means)\n",
    "    std_null = np.std(null_kappa_means, ddof=1)\n",
    "    cohen_d = (real_kappa_mean - mean_null) / std_null\n",
    "    cohen_ds[metric_name] = cohen_d\n",
    "\n",
    "#FDR CORRECTION\n",
    "pvals_list = [pvals_uncorrected[m] for m in metric_names]\n",
    "rejects, pvals_corrected = fdrcorrection(pvals_list, alpha=0.05)\n",
    "\n",
    "print(\"\\n==== FDR-corrected results (acute vs control, Î± = 0.05) ====\")\n",
    "for i, metric_name in enumerate(metric_names):\n",
    "    print(f\"{metric_name}:\\n\"\n",
    "          f\"  Real Îº = {real_kappa_means[metric_name]:.4f}\\n\"\n",
    "          f\"  Uncorrected p = {pvals_uncorrected[metric_name]:.4f}\\n\"\n",
    "          f\"  FDR-corrected p = {pvals_corrected[i]:.4f}\\n\"\n",
    "          f\"  Significant after FDR: {rejects[i]}\\n\"\n",
    "          f\"  Cohen's d = {cohen_ds[metric_name]:.2f}\\n\")\n",
    "\n",
    "#plot each metric\n",
    "for i, metric_name in enumerate(metric_names):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(null_kappa_distributions[metric_name], bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "    plt.axvline(real_kappa_means[metric_name], color='red', linestyle='dashed', linewidth=4)\n",
    "\n",
    "    plt.text(real_kappa_means[metric_name], plt.ylim()[1]*0.7,\n",
    "             f\"Real mean Îº = {real_kappa_means[metric_name]:.3f}\\n\"\n",
    "             f\"Cohen's d = {cohen_ds[metric_name]:.2f}\",\n",
    "             color='black', fontsize=10, ha='left', va='top')\n",
    "\n",
    "    plt.xlabel(\"Îº\", fontsize=20)\n",
    "    plt.ylabel(\"Frequency\", fontsize=20)\n",
    "    plt.title(f\"Permutation test: {metric_name.replace('_', ' ')} (acute vs control)\")\n",
    "    plt.tick_params(axis='x', labelsize = 20)\n",
    "    plt.tick_params(axis='y', labelsize = 20)\n",
    "    # plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the \"mixed WITHIN PAIRS\" aproach ##\n",
    "\n",
    "#real vectors from our csv files\n",
    "def load_metric_vector(subject_id, session, metric_name):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[metric_name].values\n",
    "\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects in both control and chronic: {len(common_ids)}\")\n",
    "\n",
    "metric_names = [\"Degree_centrality\", \"Closeness\", \"Clustering\"]\n",
    "\n",
    "real_kappa_means = {}\n",
    "null_kappa_distributions = {}\n",
    "pvals_uncorrected = {}\n",
    "cohen_ds = {}\n",
    "\n",
    "# loop over each metric\n",
    "for metric_name in metric_names:\n",
    "    #load metric vectors\n",
    "    control_all = []\n",
    "    chronic_all = []\n",
    "\n",
    "    for subject_id in common_ids:\n",
    "        control = load_metric_vector(subject_id, \"control\", metric_name)\n",
    "        chronic = load_metric_vector(subject_id, \"chronic\", metric_name)\n",
    "        control_all.append(control)\n",
    "        chronic_all.append(chronic)\n",
    "\n",
    "    control_all = np.array(control_all)\n",
    "    chronic_all = np.array(chronic_all)\n",
    "\n",
    "    #COMPUTE REAL KAPPAS\n",
    "    real_kappas = []\n",
    "    model = LinearRegression()\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = control_all[i].reshape(-1, 1)  \n",
    "        y = chronic_all[i] - control_all[i]\n",
    "        model.fit(x, y)\n",
    "        real_kappas.append(model.coef_[0])\n",
    "\n",
    "    real_kappas = np.array(real_kappas)\n",
    "    real_kappa_mean = np.mean(real_kappas)\n",
    "    real_kappa_means[metric_name] = real_kappa_mean\n",
    "    print(f\"{metric_name} - Real mean kappa (chronic vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    "    #PERMUTATIONS - COMPUTE \"NEW\" KAPPAS \n",
    "    n_iterations = 10000\n",
    "    null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "    for it in range(n_iterations):\n",
    "        perm_kappas = []\n",
    "\n",
    "        for i in range(len(common_ids)):\n",
    "            x = control_all[i].reshape(-1, 1) \n",
    "            delta = chronic_all[i] - control_all[i] \n",
    "\n",
    "            if np.random.rand() < 0.5:\n",
    "                delta = -delta\n",
    "\n",
    "            model.fit(x, delta)\n",
    "            perm_kappas.append(model.coef_[0])\n",
    "\n",
    "        null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "    null_kappa_distributions[metric_name] = null_kappa_means\n",
    "\n",
    "\n",
    "    p_value_two_tailed = (np.sum(np.abs(null_kappa_means) >= np.abs(real_kappa_mean)) + 1) / (n_iterations + 1)\n",
    "    pvals_uncorrected[metric_name] = p_value_two_tailed\n",
    "\n",
    "    #Cohen's d\n",
    "    mean_null = np.mean(null_kappa_means)\n",
    "    std_null = np.std(null_kappa_means, ddof=1)\n",
    "    cohen_d = (real_kappa_mean - mean_null) / std_null\n",
    "    cohen_ds[metric_name] = cohen_d\n",
    "\n",
    "#FDR CORRECTION\n",
    "pvals_list = [pvals_uncorrected[m] for m in metric_names]\n",
    "rejects, pvals_corrected = fdrcorrection(pvals_list, alpha=0.05)\n",
    "\n",
    "print(\"\\n==== FDR-corrected results (chronic vs control, Î± = 0.05) ====\")\n",
    "for i, metric_name in enumerate(metric_names):\n",
    "    print(f\"{metric_name}:\\n\"\n",
    "          f\"  Real Îº = {real_kappa_means[metric_name]:.4f}\\n\"\n",
    "          f\"  Uncorrected p = {pvals_uncorrected[metric_name]:.4f}\\n\"\n",
    "          f\"  FDR-corrected p = {pvals_corrected[i]:.4f}\\n\"\n",
    "          f\"  Significant after FDR: {rejects[i]}\\n\"\n",
    "          f\"  Cohen's d = {cohen_ds[metric_name]:.2f}\\n\")\n",
    "\n",
    "#plot each metric\n",
    "for i, metric_name in enumerate(metric_names):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(null_kappa_distributions[metric_name], bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "    plt.axvline(real_kappa_means[metric_name], color='red', linestyle='dashed', linewidth=4)\n",
    "\n",
    "    plt.text(real_kappa_means[metric_name], plt.ylim()[1]*0.7,\n",
    "             f\"Real mean Îº = {real_kappa_means[metric_name]:.3f}\\n\"\n",
    "             f\"Cohen's d = {cohen_ds[metric_name]:.2f}\",\n",
    "             color='black', fontsize=10, ha='left', va='top')\n",
    "\n",
    "    plt.xlabel(\"Îº\", fontsize=20)\n",
    "    plt.ylabel(\"Frequency\", fontsize=20)\n",
    "    plt.title(f\"Permutation test: {metric_name.replace('_', ' ')} (chronic vs control)\")\n",
    "    plt.tick_params(axis='x', labelsize = 20)\n",
    "    plt.tick_params(axis='y', labelsize = 20)\n",
    "    # plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permutations with histogram of real values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###the \"mixed WITHIN PAIRS\" aproach with the histogram of real values ##\n",
    "\n",
    "\n",
    "#real degree vectors from our csv files\n",
    "def load_degree_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Degree_centrality\"].values\n",
    "\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects in both control and chronic: {len(common_ids)}\")\n",
    "\n",
    "#load degree vectors\n",
    "deg_control_all = []\n",
    "deg_chronic_all = []\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    deg_control = load_degree_vector(subject_id, \"control\")\n",
    "    deg_chronic = load_degree_vector(subject_id, \"chronic\")\n",
    "    deg_control_all.append(deg_control)\n",
    "    deg_chronic_all.append(deg_chronic)\n",
    "\n",
    "deg_control_all = np.array(deg_control_all)\n",
    "deg_chronic_all = np.array(deg_chronic_all)\n",
    "\n",
    "#COMPUTE REAL KAPPAS\n",
    "real_kappas = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(len(common_ids)):\n",
    "    x = deg_control_all[i].reshape(-1, 1) \n",
    "    y = deg_chronic_all[i] - deg_control_all[i]\n",
    "    model.fit(x, y)\n",
    "    real_kappas.append(model.coef_[0])\n",
    "\n",
    "real_kappas = np.array(real_kappas)\n",
    "real_kappa_mean = np.mean(real_kappas)\n",
    "print(f\"Real mean kappa (chronic vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    "#PERMUTATIONS - COMPUTE \"NEW\" KAPPAS  \n",
    "n_iterations = 10000\n",
    "null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    perm_kappas = []\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = deg_control_all[i].reshape(-1, 1) \n",
    "        delta = deg_chronic_all[i] - deg_control_all[i]\n",
    "\n",
    "     \n",
    "        if np.random.rand() < 0.5:\n",
    "            delta = -delta\n",
    "\n",
    "        model.fit(x, delta)\n",
    "        perm_kappas.append(model.coef_[0])\n",
    "\n",
    "    null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "\n",
    "#load real participant kappa values\n",
    "real_kappa_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/HDI_kappa_results_within_subject/kappas_degree.csv\"\n",
    "kappa_df = pd.read_csv(real_kappa_path, sep=\",\") \n",
    "real_participant_kappas = kappa_df[\"Îº_Chronic_vs_Control\"].dropna().values  # drop NAs just in case\n",
    "\n",
    "\n",
    "# null_mean = np.mean(null_kappa_means)\n",
    "# extreme_count = np.sum(np.abs(null_kappa_means - null_mean) >= np.abs(real_kappa_mean - null_mean))\n",
    "# p_value_two_sided = extreme_count / n_iterations\n",
    "# print(f\"Two-sided P-value: {p_value_two_sided:.4f}\")\n",
    "\n",
    "\n",
    "#plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "#histogram of permuted kappa means (null distribution)\n",
    "color1 = \"darkorange\"\n",
    "counts1, bins1, patches1 = ax1.hist(real_participant_kappas, bins=12, alpha=0.8, color=color1, edgecolor=\"black\")\n",
    "ax1.set_ylabel(\"Frequency (real participants)\", color=color1, fontsize=20)\n",
    "ax1.tick_params(axis='y', labelcolor=color1, labelsize=20)\n",
    "ax1.set_ylim(0, 8)\n",
    "ax1.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "\n",
    "#secondary axis for real participants' kappa distribution\n",
    "ax2 = ax1.twinx()\n",
    "color2 = \"grey\"\n",
    "counts2, bins2, patches2 = ax2.hist(null_kappa_means, bins=20, alpha=0.6, color=color2, edgecolor=\"black\")\n",
    "ax2.axvline(real_kappa_mean, color='red', linestyle='dashed', linewidth=2, label=f\"Real Îº mean = {real_kappa_mean:.3f}\")\n",
    "ax2.set_xlabel(\"Îº value\")\n",
    "ax2.set_ylabel(\"Frequency (permutations)\", color=color2, fontsize=20)\n",
    "ax2.tick_params(axis='y', labelcolor=color2, labelsize=20)\n",
    "\n",
    "\n",
    "#add legends for both histograms\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\", fontsize=14)\n",
    "\n",
    "# # Add p-value text to plot\n",
    "# ax1.text(0.95, 0.95, f'p = {p_value_two_sided:.4f}', transform=ax1.transAxes,\n",
    "#          fontsize=12, verticalalignment='top', horizontalalignment='right',\n",
    "#          bbox=dict(facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "\n",
    "plt.title(\"Îº values distibution using degree - chronic vs control\\nPermutation null ditribution vs Real participant values\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the \"mixed WITHIN PAIRS\" aproach with the histogram of real values ##\n",
    "\n",
    "\n",
    "#real closeness vectors from our csv files\n",
    "def load_closeness_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Closeness\"].values\n",
    "\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects in both control and chronic: {len(common_ids)}\")\n",
    "\n",
    "#load degree vectors\n",
    "closeness_control_all = []\n",
    "closeness_chronic_all = []\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    closeness_control = load_closeness_vector(subject_id, \"control\")\n",
    "    closeness_chronic = load_closeness_vector(subject_id, \"chronic\")\n",
    "    closeness_control_all.append(closeness_control)\n",
    "    closeness_chronic_all.append(closeness_chronic)\n",
    "\n",
    "closeness_control_all = np.array(closeness_control_all)\n",
    "closeness_chronic_all = np.array(closeness_chronic_all)\n",
    "\n",
    "#COMPUTE REAL KAPPAS\n",
    "real_kappas = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(len(common_ids)):\n",
    "    x = closeness_control_all[i].reshape(-1, 1)  \n",
    "    y = closeness_chronic_all[i] - closeness_control_all[i]\n",
    "    model.fit(x, y)\n",
    "    real_kappas.append(model.coef_[0])\n",
    "\n",
    "real_kappas = np.array(real_kappas)\n",
    "real_kappa_mean = np.mean(real_kappas)\n",
    "print(f\"Real mean kappa (chronic vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    "#PERMUTATIONS - COMPUTE \"NEW\" KAPPAS \n",
    "n_iterations = 10000\n",
    "null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    perm_kappas = []\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = closeness_control_all[i].reshape(-1, 1) \n",
    "        delta = closeness_chronic_all[i] - closeness_control_all[i]\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            delta = -delta\n",
    "\n",
    "        model.fit(x, delta)\n",
    "        perm_kappas.append(model.coef_[0])\n",
    "\n",
    "    null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "\n",
    "# Load real participant kappa values\n",
    "real_kappa_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/HDI_kappa_results_within_subject/kappas_closeness.csv\"\n",
    "kappa_df = pd.read_csv(real_kappa_path, sep=\",\")  # Assuming it's tab-separated\n",
    "real_participant_kappas = kappa_df[\"Chronic_vs_Control\"].dropna().values  # drop NAs just in case\n",
    "\n",
    "\n",
    "# null_mean = np.mean(null_kappa_means)\n",
    "# extreme_count = np.sum(np.abs(null_kappa_means - null_mean) >= np.abs(real_kappa_mean - null_mean))\n",
    "# p_value_two_sided = extreme_count / n_iterations\n",
    "# print(f\"Two-sided P-value: {p_value_two_sided:.4f}\")\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Histogram of permuted kappa means (null distribution)\n",
    "color1 = \"darkorange\"\n",
    "counts1, bins1, patches1 = ax1.hist(real_participant_kappas, bins=12, alpha=0.8, color=color1, edgecolor=\"black\")\n",
    "ax1.set_ylabel(\"Frequency (real participants)\", color=color1, fontsize=20)\n",
    "ax1.tick_params(axis='y', labelcolor=color1, labelsize=20)\n",
    "ax1.set_ylim(0, 8)\n",
    "ax1.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "\n",
    "# Secondary axis for real participants' kappa distribution\n",
    "ax2 = ax1.twinx()\n",
    "color2 = \"grey\"\n",
    "counts2, bins2, patches2 = ax2.hist(null_kappa_means, bins=20, alpha=0.6, color=color2, edgecolor=\"black\")\n",
    "ax2.axvline(real_kappa_mean, color='red', linestyle='dashed', linewidth=2, label=f\"Real Îº mean = {real_kappa_mean:.3f}\")\n",
    "ax2.set_xlabel(\"Îº value\")\n",
    "ax2.set_ylabel(\"Frequency (permutations)\", color=color2, fontsize=20)\n",
    "ax2.tick_params(axis='y', labelcolor=color2, labelsize=20)\n",
    "\n",
    "\n",
    "# Add legends for both histograms\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\", fontsize=14)\n",
    "\n",
    "# # Add p-value text to plot\n",
    "# ax1.text(0.95, 0.95, f'p = {p_value_two_sided:.4f}', transform=ax1.transAxes,\n",
    "#          fontsize=12, verticalalignment='top', horizontalalignment='right',\n",
    "#          bbox=dict(facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "\n",
    "plt.title(\"Îº values distibution using closeness - chronic vs control\\nPermutation null ditribution vs Real participant values\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###the \"mixed WITHIN PAIRS\" aproach with the histogram of real values ##\n",
    "\n",
    "#real clustering vectors from our csv files\n",
    "def load_clustering_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Clustering\"].values\n",
    "\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"acute\": os.path.join(base_path, \"ses-2\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses2 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"acute\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses2))\n",
    "\n",
    "print(f\"Subjects in both control and acute: {len(common_ids)}\")\n",
    "\n",
    "#load degree vectors\n",
    "clustering_control_all = []\n",
    "clustering_acute_all = []\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    clustering_control = load_clustering_vector(subject_id, \"control\")\n",
    "    clustering_acute = load_clustering_vector(subject_id, \"acute\")\n",
    "    clustering_control_all.append(clustering_control)\n",
    "    clustering_acute_all.append(clustering_acute)\n",
    "\n",
    "clustering_control_all = np.array(clustering_control_all)\n",
    "clustering_acute_all = np.array(clustering_acute_all)\n",
    "\n",
    "#COMPUTE REAL KAPPAS\n",
    "real_kappas = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(len(common_ids)):\n",
    "    x = clustering_control_all[i].reshape(-1, 1)  \n",
    "    y = clustering_acute_all[i] - clustering_control_all[i]\n",
    "    model.fit(x, y)\n",
    "    real_kappas.append(model.coef_[0])\n",
    "\n",
    "real_kappas = np.array(real_kappas)\n",
    "real_kappa_mean = np.mean(real_kappas)\n",
    "print(f\"Real mean kappa (acute vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    "#PERMUTATIONS - COMPUTE \"NEW\" KAPPAS \n",
    "n_iterations = 10000\n",
    "null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    perm_kappas = []\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = clustering_control_all[i].reshape(-1, 1) \n",
    "        delta = clustering_acute_all[i] - clustering_control_all[i] \n",
    "\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            delta = -delta\n",
    "\n",
    "        model.fit(x, delta)\n",
    "        perm_kappas.append(model.coef_[0])\n",
    "\n",
    "    null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "\n",
    "# Load real participant kappa values\n",
    "real_kappa_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/HDI_kappa_results_within_subject/kappas_clustering.csv\"\n",
    "kappa_df = pd.read_csv(real_kappa_path, sep=\",\") \n",
    "real_participant_kappas = kappa_df[\"Acute_vs_Control\"].dropna().values  \n",
    "\n",
    "\n",
    "# null_mean = np.mean(null_kappa_means)\n",
    "# extreme_count = np.sum(np.abs(null_kappa_means - null_mean) >= np.abs(real_kappa_mean - null_mean))\n",
    "# p_value_two_sided = extreme_count / n_iterations\n",
    "# print(f\"Two-sided P-value: {p_value_two_sided:.4f}\")\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Histogram of permuted kappa means (null distribution)\n",
    "color1 = \"darkorange\"\n",
    "counts1, bins1, patches1 = ax1.hist(real_participant_kappas, bins=12, alpha=0.8, color=color1, edgecolor=\"black\")\n",
    "ax1.set_ylabel(\"Frequency (real participants)\", color=color1, fontsize=20)\n",
    "ax1.tick_params(axis='y', labelcolor=color1, labelsize=20)\n",
    "ax1.set_ylim(0, 8)\n",
    "ax1.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "\n",
    "# Secondary axis for real participants' kappa distribution\n",
    "ax2 = ax1.twinx()\n",
    "color2 = \"grey\"\n",
    "counts2, bins2, patches2 = ax2.hist(null_kappa_means, bins=20, alpha=0.6, color=color2, edgecolor=\"black\")\n",
    "ax2.axvline(real_kappa_mean, color='red', linestyle='dashed', linewidth=2, label=f\"Real Îº mean = {real_kappa_mean:.3f}\")\n",
    "ax2.set_xlabel(\"Îº value\")\n",
    "ax2.set_ylabel(\"Frequency (permutations)\", color=color2, fontsize=20)\n",
    "ax2.tick_params(axis='y', labelcolor=color2, labelsize=20)\n",
    "\n",
    "\n",
    "# Add legends for both histograms\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\", fontsize=14)\n",
    "\n",
    "# # Add p-value text to plot\n",
    "# ax1.text(0.95, 0.95, f'p = {p_value_two_sided:.4f}', transform=ax1.transAxes,\n",
    "#          fontsize=12, verticalalignment='top', horizontalalignment='right',\n",
    "#          bbox=dict(facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "\n",
    "plt.title(\"Îº values distibution using clustering - acute vs control\\nPermutation null ditribution vs Real participant values\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "sessions = {\n",
    "    \"control\": \"ses-1\",\n",
    "    \"acute\": \"ses-2\",\n",
    "    \"chronic\": \"ses-3\"\n",
    "}\n",
    "metrics_list = [\"Degree_centrality\", \"Closeness\", \"Clustering\"]\n",
    "session_pairs = [\n",
    "    (\"acute\", \"control\"),\n",
    "    (\"chronic\", \"control\"),\n",
    "    (\"chronic\", \"acute\")\n",
    "]\n",
    "# Custom labels for each comparison\n",
    "comparison_labels = {\n",
    "    (\"acute\", \"control\"): \"TSD vs RW\",\n",
    "    (\"chronic\", \"control\"): \"CSR vs RW\",\n",
    "    (\"chronic\", \"acute\"): \"CSR vs TSD\"\n",
    "}\n",
    "\n",
    "n_iterations = 10000\n",
    "\n",
    "def load_metric_vector(subject_id, session, metric):\n",
    "    folder = os.path.join(base_path, sessions[session], subject_id)\n",
    "    pattern = os.path.join(folder, \"Graphs/wAALours/graph_metrics/*_metrics.csv\")\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[metric].values\n",
    "\n",
    "# Initialize plot with metrics as rows, comparisons as columns\n",
    "fig, axes = plt.subplots(len(metrics_list), len(session_pairs), figsize=(18, 12))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Loop over all combinations\n",
    "for col_idx, (ses1, ses_ref) in enumerate(session_pairs):\n",
    "    ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(base_path, sessions[ses1], \"sub-*\"))}\n",
    "    ids_ses_ref = {os.path.basename(p) for p in glob.glob(os.path.join(base_path, sessions[ses_ref], \"sub-*\"))}\n",
    "    common_ids = sorted(list(ids_ses1 & ids_ses_ref))\n",
    "    print(f\"{ses1} vs {ses_ref}: {len(common_ids)} subjects\")\n",
    "\n",
    "    for row_idx, metric in enumerate(metrics_list):\n",
    "        vec_ref_all = []\n",
    "        vec_test_all = []\n",
    "\n",
    "        for subject_id in common_ids:\n",
    "            vec_ref = load_metric_vector(subject_id, ses_ref, metric)\n",
    "            vec_test = load_metric_vector(subject_id, ses1, metric)\n",
    "            vec_ref_all.append(vec_ref)\n",
    "            vec_test_all.append(vec_test)\n",
    "\n",
    "        vec_ref_all = np.array(vec_ref_all)\n",
    "        vec_test_all = np.array(vec_test_all)\n",
    "        group_ref_mean = np.mean(vec_ref_all, axis=0)\n",
    "\n",
    "        # Real kappa differences\n",
    "        model = LinearRegression()\n",
    "        real_diffs = []\n",
    "        for i in range(len(common_ids)):\n",
    "            delta_within = vec_test_all[i] - vec_ref_all[i]\n",
    "            model.fit(vec_ref_all[i].reshape(-1, 1), delta_within)\n",
    "            kappa_within = model.coef_[0]\n",
    "\n",
    "            delta_group = vec_test_all[i] - group_ref_mean\n",
    "            model.fit(group_ref_mean.reshape(-1, 1), delta_group)\n",
    "            kappa_group = model.coef_[0]\n",
    "\n",
    "            real_diffs.append(kappa_within - kappa_group)\n",
    "\n",
    "        real_diffs = np.array(real_diffs)\n",
    "        real_mean_diff = np.mean(real_diffs)\n",
    "\n",
    "        # Permutations\n",
    "        null_diffs = np.zeros(n_iterations)\n",
    "        for it in range(n_iterations):\n",
    "            diffs = []\n",
    "            for i in range(len(common_ids)):\n",
    "                perm_test = np.random.permutation(vec_test_all[i])\n",
    "                delta_within = perm_test - vec_ref_all[i]\n",
    "                model.fit(vec_ref_all[i].reshape(-1, 1), delta_within)\n",
    "                kappa_within = model.coef_[0]\n",
    "\n",
    "                delta_group = perm_test - group_ref_mean\n",
    "                model.fit(group_ref_mean.reshape(-1, 1), delta_group)\n",
    "                kappa_group = model.coef_[0]\n",
    "\n",
    "                diffs.append(kappa_within - kappa_group)\n",
    "            null_diffs[it] = np.mean(diffs)\n",
    "\n",
    "        # P-value\n",
    "        p_value = np.mean(null_diffs <= real_mean_diff)\n",
    "\n",
    "        # Plot\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        ax.hist(null_diffs, bins=30, alpha=0.7, color='grey')\n",
    "        ax.axvline(real_mean_diff, color='red', linestyle='dashed', linewidth=2,\n",
    "                   label=f\"Real = {real_mean_diff:.3f}\\nP = {p_value:.4f}\")\n",
    "        ax.set_title(f\"{comparison_labels[(ses1, ses_ref)]}\", fontsize=10)\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{metric}\\nFrequency\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"Îº_within âˆ’ Îº_group\")\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle(\"Permutation Test: Îº_within âˆ’ Îº_groupmean\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "sessions = {\n",
    "    \"control\": \"ses-1\",\n",
    "    \"acute\": \"ses-2\",\n",
    "    \"chronic\": \"ses-3\"\n",
    "}\n",
    "metrics_list = [\"Degree_centrality\", \"Closeness\", \"Clustering\"]\n",
    "session_pairs = [\n",
    "    (\"acute\", \"control\"),\n",
    "    (\"chronic\", \"control\"),\n",
    "    (\"chronic\", \"acute\")\n",
    "]\n",
    "# Custom labels\n",
    "comparison_labels = {\n",
    "    (\"acute\", \"control\"): \"TSD vs RW\",\n",
    "    (\"chronic\", \"control\"): \"CSR vs RW\",\n",
    "    (\"chronic\", \"acute\"): \"CSR vs TSD\"\n",
    "}\n",
    "\n",
    "n_iterations = 10000\n",
    "\n",
    "# Load metric vector\n",
    "def load_metric_vector(subject_id, session, metric):\n",
    "    folder = os.path.join(base_path, sessions[session], subject_id)\n",
    "    pattern = os.path.join(folder, \"Graphs/wAALours/graph_metrics/*_metrics.csv\")\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[metric].values\n",
    "\n",
    "# Create subplot: metrics = rows, comparisons = columns\n",
    "fig, axes = plt.subplots(len(metrics_list), len(session_pairs), figsize=(18, 12))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Loop over each comparison and metric\n",
    "for col_idx, (ses1, ses_ref) in enumerate(session_pairs):\n",
    "    ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(base_path, sessions[ses1], \"sub-*\"))}\n",
    "    ids_ses_ref = {os.path.basename(p) for p in glob.glob(os.path.join(base_path, sessions[ses_ref], \"sub-*\"))}\n",
    "    common_ids = sorted(list(ids_ses1 & ids_ses_ref))\n",
    "    print(f\"{ses1} vs {ses_ref}: {len(common_ids)} subjects\")\n",
    "\n",
    "    for row_idx, metric in enumerate(metrics_list):\n",
    "        vec_ref_all = []\n",
    "        vec_test_all = []\n",
    "\n",
    "        for subject_id in common_ids:\n",
    "            vec_ref = load_metric_vector(subject_id, ses_ref, metric)\n",
    "            vec_test = load_metric_vector(subject_id, ses1, metric)\n",
    "            vec_ref_all.append(vec_ref)\n",
    "            vec_test_all.append(vec_test)\n",
    "\n",
    "        vec_ref_all = np.array(vec_ref_all)\n",
    "        vec_test_all = np.array(vec_test_all)\n",
    "\n",
    "        # Compute real kappas\n",
    "        model = LinearRegression()\n",
    "        real_kappas = []\n",
    "        for i in range(len(common_ids)):\n",
    "            x = vec_ref_all[i].reshape(-1, 1)\n",
    "            y = vec_test_all[i] - vec_ref_all[i]\n",
    "            model.fit(x, y)\n",
    "            real_kappas.append(model.coef_[0])\n",
    "        real_kappas = np.array(real_kappas)\n",
    "        real_kappa_mean = np.mean(real_kappas)\n",
    "\n",
    "        # Permutation: mix subjects\n",
    "        null_kappa_means = np.zeros(n_iterations)\n",
    "        for it in range(n_iterations):\n",
    "            perm_kappas = []\n",
    "            perm_indices = np.random.permutation(len(common_ids))\n",
    "            for i in range(len(common_ids)):\n",
    "                x = vec_ref_all[i].reshape(-1, 1)\n",
    "                y = vec_test_all[perm_indices[i]] - vec_ref_all[i]\n",
    "                model.fit(x, y)\n",
    "                perm_kappas.append(model.coef_[0])\n",
    "            null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "        # P-value\n",
    "        # p_value = np.mean(null_kappa_means <= real_kappa_mean)\n",
    "        # Two-tailed p-value\n",
    "        null_mean = np.mean(null_kappa_means)\n",
    "        p_value = np.mean(np.abs(null_kappa_means - null_mean) >= np.abs(real_kappa_mean - null_mean))\n",
    "\n",
    "\n",
    "        # Plotting\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        ax.hist(null_kappa_means, bins=30, alpha=0.7, color='grey')\n",
    "        ax.axvline(real_kappa_mean, color='red', linestyle='dashed', linewidth=2,\n",
    "                   label=f\"Real Îº = {real_kappa_mean:.3f}\\nP = {p_value:.4f}\")\n",
    "        ax.set_title(f\"{comparison_labels[(ses1, ses_ref)]}\", fontsize=10)\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{metric}\\nFrequency\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"Mean Îº (shuffled)\")\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle(\"Permutation Test: Mean Îº (unpaired)\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the \"mix the order of patients\" aproach ##\n",
    "\n",
    "#real degree vectors from our csv files\n",
    "def load_degree_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Degree_centrality\"].values\n",
    "\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "#subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects in both control and chronic: {len(common_ids)}\")\n",
    "\n",
    "#load degree vectors\n",
    "deg_control_all = []\n",
    "deg_chronic_all = []\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    deg_control = load_degree_vector(subject_id, \"control\")\n",
    "    deg_chronic = load_degree_vector(subject_id, \"chronic\")\n",
    "    deg_control_all.append(deg_control)\n",
    "    deg_chronic_all.append(deg_chronic)\n",
    "\n",
    "deg_control_all = np.array(deg_control_all)\n",
    "deg_chronic_all = np.array(deg_chronic_all)\n",
    "\n",
    "#COMPUTE REAL KAPPAS\n",
    "real_kappas = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(len(common_ids)):\n",
    "    x = deg_control_all[i].reshape(-1, 1) \n",
    "    y = deg_chronic_all[i] - deg_control_all[i]\n",
    "    model.fit(x, y)\n",
    "    real_kappas.append(model.coef_[0])\n",
    "\n",
    "real_kappas = np.array(real_kappas)\n",
    "real_kappa_mean = np.mean(real_kappas)\n",
    "print(f\"Real mean kappa (chronic vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    "#PERMUTATIONS - RANDOMLY BREAK SUBJECT PAIRINGS BETWEEN CHRONIC AND CONTROL\n",
    "n_iterations = 10000\n",
    "null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    perm_kappas = []\n",
    "\n",
    "    # random permutation of chronic subjects (break the subject pairing)\n",
    "    permuted_chronic_indices = np.random.permutation(len(common_ids))\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = deg_control_all[i].reshape(-1, 1)\n",
    "        y = deg_chronic_all[permuted_chronic_indices[i]] - deg_control_all[i]\n",
    "\n",
    "        \n",
    "        model.fit(x, y)\n",
    "        perm_kappas.append(model.coef_[0])\n",
    "\n",
    "    null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "\n",
    "#plot \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(null_kappa_means, bins=30, alpha=0.7)\n",
    "plt.axvline(real_kappa_mean, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f\"Real mean Îº = {real_kappa_mean:.3f}\")\n",
    "plt.xlabel(\"Mean kappa (permutation distribution)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Permutations - distribution of mean kappa (chronic vs control) using degree\\n (controls in correct order, chronics random)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the \"mixed nodes\" aproach ##\n",
    "\n",
    "#real degree vectors from our csv files\n",
    "def load_degree_vector(subject_id, session):\n",
    "    folder = os.path.join(ses_paths[session], subject_id)\n",
    "    files = glob.glob(os.path.join(folder, metrics_pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No metrics file found for {subject_id} in {session}\")\n",
    "    df = pd.read_csv(files[0])\n",
    "    return df[\"Degree_centrality\"].values\n",
    "\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "ses_paths = {\n",
    "    \"control\": os.path.join(base_path, \"ses-1\"),\n",
    "    \"chronic\": os.path.join(base_path, \"ses-3\")\n",
    "}\n",
    "metrics_pattern = \"Graphs/wAALours/graph_metrics/*_metrics.csv\"\n",
    "\n",
    "# subject IDs present in both sessions\n",
    "ids_ses1 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"control\"], \"sub-*\"))}\n",
    "ids_ses3 = {os.path.basename(p) for p in glob.glob(os.path.join(ses_paths[\"chronic\"], \"sub-*\"))}\n",
    "common_ids = sorted(list(ids_ses1 & ids_ses3))\n",
    "\n",
    "print(f\"Subjects in both control and chronic: {len(common_ids)}\")\n",
    "\n",
    "# load degree vectors\n",
    "deg_control_all = []\n",
    "deg_chronic_all = []\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    deg_control = load_degree_vector(subject_id, \"control\")\n",
    "    deg_chronic = load_degree_vector(subject_id, \"chronic\")\n",
    "    deg_control_all.append(deg_control)\n",
    "    deg_chronic_all.append(deg_chronic)\n",
    "\n",
    "deg_control_all = np.array(deg_control_all)\n",
    "deg_chronic_all = np.array(deg_chronic_all)\n",
    "\n",
    "# COMPUTE REAL KAPPAS\n",
    "real_kappas = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(len(common_ids)):\n",
    "    x = deg_control_all[i].reshape(-1, 1) \n",
    "    y = deg_chronic_all[i] - deg_control_all[i]\n",
    "    model.fit(x, y)\n",
    "    real_kappas.append(model.coef_[0])\n",
    "\n",
    "real_kappas = np.array(real_kappas)\n",
    "real_kappa_mean = np.mean(real_kappas)\n",
    "print(f\"Real mean kappa (chronic vs control): {real_kappa_mean:.4f}\")\n",
    "\n",
    "# PERMUTATIONS - shuffle node order in ONE of the vectors (e.g., chronic)\n",
    "n_iterations = 10000\n",
    "null_kappa_means = np.zeros(n_iterations)\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    perm_kappas = []\n",
    "\n",
    "    for i in range(len(common_ids)):\n",
    "        x = deg_control_all[i]  # do not shuffle\n",
    "        y = deg_chronic_all[i].copy()\n",
    "        \n",
    "        #shuffle node order in chronic session\n",
    "        y_shuffled = np.random.permutation(y)\n",
    "\n",
    "        delta = y_shuffled - x  #mismatch in node correspondence, break the pairs\n",
    "        model.fit(x.reshape(-1, 1), delta)\n",
    "        perm_kappas.append(model.coef_[0])\n",
    "\n",
    "    null_kappa_means[it] = np.mean(perm_kappas)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(null_kappa_means, bins=30, alpha=0.7)\n",
    "plt.axvline(real_kappa_mean, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f\"Real Îº = {real_kappa_mean:.3f}\")\n",
    "plt.xlabel(\"Mean kappa (permutation distribution)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Permutations - distribution of mean kappa (chronic vs control) using degree\\n(node-wise shuffling WITHIN pairs)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariate constraint manifold learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCML degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Degree_centrality\"\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "metric_filename_pattern = \"*_metrics.csv\"\n",
    "\n",
    "#kappas file\n",
    "kappa_all = pd.read_csv(os.path.join(base_path, \"HDI_kappa_results_within_subject\", \"kappas_degree.csv\"))\n",
    "kappa_all.set_index(\"Subject_ID\", inplace=True)\n",
    "\n",
    "#subject directories\n",
    "control_dir = os.path.join(base_path, \"ses-2\") \n",
    "patient_dir = os.path.join(base_path, \"ses-3\")  \n",
    "\n",
    "control_subjects = sorted(glob.glob(os.path.join(control_dir, \"sub-*\")))\n",
    "patient_subjects = sorted(glob.glob(os.path.join(patient_dir, \"sub-*\")))\n",
    "\n",
    "#extract the metric for each subject\n",
    "def extract_metric(subject_paths):\n",
    "    data = []\n",
    "    ids = []\n",
    "    for subj_path in subject_paths:\n",
    "        pattern = os.path.join(subj_path, \"Graphs/wAALours/graph_metrics\", metric_filename_pattern)\n",
    "        files = glob.glob(pattern)\n",
    "        if not files:\n",
    "            continue\n",
    "        df = pd.read_csv(files[0])\n",
    "        if metric not in df.columns:\n",
    "            continue\n",
    "        data.append(df[metric].values)\n",
    "        ids.append(os.path.basename(subj_path))\n",
    "    return np.array(data), ids\n",
    "\n",
    "#extract data\n",
    "control_data, control_ids = extract_metric(control_subjects)\n",
    "patient_data, patient_ids = extract_metric(patient_subjects)\n",
    "\n",
    "#just in case - clean subject IDs to match kappa file format\n",
    "control_ids_cleaned = [cid.replace(\"sub-\", \"\") for cid in control_ids]\n",
    "patient_ids_cleaned = [pid.replace(\"sub-\", \"\") for pid in patient_ids]\n",
    "\n",
    "#extract matching kappa values\n",
    "kappa_control_values = kappa_all.loc[[\"sub-\" + cid for cid in control_ids_cleaned], \"Îº_Acute_vs_Control\"].values\n",
    "kappa_patient_values = kappa_all.loc[[\"sub-\" + pid for pid in patient_ids_cleaned], \"Îº_Chronic_vs_Control\"].values\n",
    "\n",
    "#construct covariate vector\n",
    "cov = np.concatenate([kappa_control_values, kappa_patient_values])\n",
    "\n",
    "#group labels: 0 = control, 1 = patient\n",
    "labels = [0] * len(control_ids) + [1] * len(patient_ids)\n",
    "\n",
    "#subject names \n",
    "subject_names = [f\"T{i}\" for i in range(len(control_ids))] + [f\"C{i}\" for i in range(len(patient_ids))]\n",
    "\n",
    "#combine metric data\n",
    "X = np.vstack([control_data, patient_data])\n",
    "\n",
    "#save outputs for CCML (exactly the same format as in Sophie's tutorial code)\n",
    "output_dir = os.path.join(base_path, \"CCML_inputs_within_subjects\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "df_X = pd.DataFrame(X, index=subject_names)\n",
    "df_X.to_csv(os.path.join(output_dir, f\"{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "\n",
    "df_cov = pd.DataFrame(cov, index=subject_names, columns=[\"HDI_kappa\"])\n",
    "df_cov.to_csv(os.path.join(output_dir, f\"HDI_kappa_{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "\n",
    "df_labels = pd.DataFrame(labels, index=subject_names, columns=[\"Group\"])\n",
    "df_labels.to_csv(os.path.join(output_dir, f\"group_labels_{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "print(\"Files for CCML saved in:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_coord1(x,X2,cov2,alpha2,dist2):\n",
    "    X2 = np.vstack([X2,x])\n",
    "    cov2 = cov2.reshape(X2.shape)\n",
    "    X_tmp2 = np.hstack((alpha2*cov2,X2))\n",
    "    D2 = sk.metrics.pairwise_distances(X_tmp2)\n",
    "    return np.linalg.norm(dist2-D2)\n",
    "\n",
    "def f_glob(X2,cov2,alpha2,dist2):\n",
    "    cov2 = cov2.reshape(X2.shape)\n",
    "    X_tmp2 = np.vstack((alpha2*cov2,X2)).T\n",
    "    D2 = sk.metrics.pairwise_distances(X_tmp2)\n",
    "    return np.linalg.norm(dist2-D2)\n",
    "\n",
    "def f_alpha(alpha,X2,cov2,dist2):\n",
    "    cov2 = cov2.reshape(X2.shape)\n",
    "    X_tmp = np.vstack((alpha*cov2,X2))\n",
    "    D2 = sk.metrics.pairwise_distances(X_tmp.T)\n",
    "    return np.linalg.norm(dist2-D2)\n",
    "\n",
    "dfX = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/CCML_inputs_within_subjects/Degree_centrality_chronic_and_acute_vs_control.csv\", index_col=0)\n",
    "X = dfX.to_numpy()\n",
    "\n",
    "dfcov = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/CCML_inputs_within_subjects/HDI_kappa_Degree_centrality_chronic_and_acute_vs_control.csv\", index_col=0)\n",
    "cov = dfcov.to_numpy()\n",
    "\n",
    "\n",
    "labels_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/CCML_inputs_within_subjects/group_labels_Degree_centrality_chronic_and_acute_vs_control.csv\"\n",
    "labels_df = pd.read_csv(labels_path, index_col=0)\n",
    "\n",
    "\n",
    "Indiv = labels_df.index.tolist()\n",
    "l = labels_df[\"Group\"].values\n",
    "\n",
    "#compute Euclidean distance matrix\n",
    "D = sk.metrics.pairwise.pairwise_distances(X)\n",
    "\n",
    "#initialisation aleatoire\n",
    "#list_extract = sk.utils.shuffle(np.arange(X.shape[0]))\n",
    "\n",
    "#initialize with the most remote couple of points\n",
    "ll = []\n",
    "ll.append(np.where(D==np.max(D))[0][0])\n",
    "ll.append(np.where(D==np.max(D))[1][0])\n",
    "\n",
    "# Add sequentially the furthest point of the previous set\n",
    "while(len(ll)!=D.shape[0]):\n",
    "    D_tmp = D[ll]\n",
    "    D_tmp[:,ll] = 0\n",
    "    ll.append(np.where(D_tmp == np.max(D_tmp))[1][0])\n",
    "\n",
    "list_extract = np.array(ll)\n",
    "\n",
    "# Initialize alpha parameter with the classic Isomap\n",
    "iso = sk.manifold.Isomap(n_components=2,n_neighbors=4)\n",
    "iso.fit(X)\n",
    "tmp = iso.embedding_[:,0]\n",
    "dist = iso.dist_matrix_\n",
    "delta_1 = np.max(tmp) -np.min(tmp) \n",
    "delta_2 = np.max(cov) -np.min(cov)\n",
    "alpha = delta_1/delta_2\n",
    "\n",
    "#Initialize the first point at 0\n",
    "X_tmp = np.zeros([1])\n",
    "\n",
    "\n",
    "for i in range(1,X.shape[0]):\n",
    "    print(\"one coord\",i)\n",
    "    cov_tmp = cov[list_extract[0:i+1]]\n",
    "    dist_tmp = dist[list_extract[0:i+1]][:,list_extract[0:i+1]]\n",
    "    opt_test = 10000    \n",
    "    #Multi-start opitmisation\n",
    "    for j in range(-5,5):\n",
    "        xtmptmp,B,tmp,tmp1,tmp3 = opt.fmin(f_coord1,j,(X_tmp,cov_tmp,alpha,dist_tmp),xtol=0.000000001, ftol=0.000000001,maxiter=1000000,maxfun=1000000,disp = 0 , full_output = 1)\n",
    "        if opt_test>B:\n",
    "            xtmp = xtmptmp\n",
    "            opt_test = B\n",
    "            print(j)\n",
    "    X_tmp = np.vstack([X_tmp, xtmp])\n",
    "# We can add an additional global optimisation before the estimation of the alpha coefficient\n",
    "#    X_tmp2,B,tmp,tmp1,tmp3 = opt.fmin(f_glob,X_tmp,(cov_tmp,alpha,dist_tmp), xtol=0.000000001, ftol=0.000000001,maxiter=1000000,maxfun=1000000,disp = 1 , full_output = 1)\n",
    "#    print \"all_coord\"\n",
    "#    X_tmp = X_tmp2.reshape(-1,1)\n",
    "    alpha,B,tmp,tmp1,tmp3 = opt.fmin(f_alpha,alpha,(X_tmp,cov_tmp,dist_tmp), xtol=0.000000001, ftol=0.00000001,disp = 1 , full_output = 1)\n",
    "    X_tmp2,B,tmp,tmp1,tmp3 = opt.fmin(f_glob,X_tmp,(cov_tmp,alpha,dist_tmp), xtol=0.000000000001, ftol=0.00000000001,maxiter=1000000,maxfun=1000000,disp = 1 , full_output = 1)\n",
    "    print(\"all_coord\")\n",
    "    X_tmp = X_tmp2.reshape(-1,1)\n",
    "\n",
    "# Merge the covariate modulated by alpha and the first component\n",
    "cov2 = cov.reshape(X_tmp.shape)\n",
    "X_iso = np.hstack((alpha*cov2[list_extract],X_tmp))\n",
    "\n",
    "# Re-order the rows of the matrix to correspond to the initial order of the subject\n",
    "I = np.argsort(list_extract)\n",
    "X_iso_f = X_iso[I]  \n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/patrycjascislewska/Analizy_neuro/Graphs/tutorials/TP2_Graphs_ILCB_summer_School')\n",
    "\n",
    "import PlotFunction as pf\n",
    "\n",
    "###### Visualization\n",
    "\n",
    "Xt =  cov\n",
    "scaling = 5\n",
    "\n",
    "grid_x , grid_y = np.mgrid[-1:1:1000j,-1:1:1000j]*scaling\n",
    "grid_lin = griddata(X_iso_f,Xt,(grid_x,grid_y),method='linear')\n",
    "grid_lin = grid_lin.reshape(1000,1000)\n",
    "pf.scatter_2D(X_iso_f, l,Indiv)\n",
    "\n",
    "#plt.imshow(grid_lin.T, extent=(-1,1,-1,1), origin='lower')\n",
    "plt.imshow(grid_lin.T,extent=(-scaling,scaling,-scaling,scaling), origin='lower')\n",
    "plt.colorbar().ax.tick_params(labelsize=20)\n",
    "plt.title('Degree: TSD and CSR compared to RW', fontsize=16)\n",
    "\n",
    "plt.tick_params(labelsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of difference between two clusters in CCML - for acute and chronic\n",
    "\n",
    "group_acute = labels_df[\"Group\"] == 0\n",
    "group_chronic = labels_df[\"Group\"] == 1\n",
    "\n",
    "#extract group coordinates\n",
    "X_acute = X_iso_f[group_acute.values]\n",
    "X_chronic = X_iso_f[group_chronic.values]\n",
    "\n",
    "#calculate observed centroid distance - distance between two clusters \n",
    "d_obs = np.linalg.norm(X_acute.mean(axis=0) - X_chronic.mean(axis=0))\n",
    "\n",
    "#permutations\n",
    "n_perm = 10000\n",
    "d_perm = []\n",
    "group_labels = labels_df[\"Group\"].values\n",
    "\n",
    "for _ in range(n_perm):\n",
    "    perm_labels = np.random.permutation(group_labels) #randomly mix the labels of acutes and chronics\n",
    "    perm_acute = X_iso_f[perm_labels == 0]\n",
    "    perm_chronic = X_iso_f[perm_labels == 1]\n",
    "    d = np.linalg.norm(perm_acute.mean(axis=0) - perm_chronic.mean(axis=0))\n",
    "    d_perm.append(d)\n",
    "\n",
    "#p-value\n",
    "p_val = np.mean(np.array(d_perm) >= d_obs) #greater than or equal to the observed distance\n",
    "\n",
    "print(f\"Observed centroid distance = {d_obs:.4f}\")\n",
    "print(f\"P-value (permutation test) = {p_val:.4f}\")\n",
    "\n",
    "\n",
    "#histogram of permuted distances\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(d_perm, bins=50, edgecolor='black', alpha=0.7, color='grey')\n",
    "plt.axvline(d_obs, color='red', linestyle='--', linewidth=2, label=f'Observed distance = {d_obs:.3f}\\np-value = {p_val:.4f}')\n",
    "plt.title(\"Permutation test - centroid distance\", fontsize=20)\n",
    "plt.xlabel(\"Centroid distance\", fontsize=18)\n",
    "plt.ylabel(\"Frequency\", fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCML closeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Closeness\"\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "metric_filename_pattern = \"*_metrics.csv\"\n",
    "\n",
    "#kappas file\n",
    "kappa_all = pd.read_csv(os.path.join(base_path, \"HDI_kappa_results_within_subject\", \"kappas_closeness.csv\"))\n",
    "kappa_all.set_index(\"Subject_ID\", inplace=True)\n",
    "\n",
    "#subject directories\n",
    "control_dir = os.path.join(base_path, \"ses-2\") \n",
    "patient_dir = os.path.join(base_path, \"ses-3\")  \n",
    "\n",
    "control_subjects = sorted(glob.glob(os.path.join(control_dir, \"sub-*\")))\n",
    "patient_subjects = sorted(glob.glob(os.path.join(patient_dir, \"sub-*\")))\n",
    "\n",
    "#extract the metric for each subject\n",
    "def extract_metric(subject_paths):\n",
    "    data = []\n",
    "    ids = []\n",
    "    for subj_path in subject_paths:\n",
    "        pattern = os.path.join(subj_path, \"Graphs/wAALours/graph_metrics\", metric_filename_pattern)\n",
    "        files = glob.glob(pattern)\n",
    "        if not files:\n",
    "            continue\n",
    "        df = pd.read_csv(files[0])\n",
    "        if metric not in df.columns:\n",
    "            continue\n",
    "        data.append(df[metric].values)\n",
    "        ids.append(os.path.basename(subj_path))\n",
    "    return np.array(data), ids\n",
    "\n",
    "#extract data\n",
    "control_data, control_ids = extract_metric(control_subjects)\n",
    "patient_data, patient_ids = extract_metric(patient_subjects)\n",
    "\n",
    "#just in case - clean subject IDs to match kappa file format\n",
    "control_ids_cleaned = [cid.replace(\"sub-\", \"\") for cid in control_ids]\n",
    "patient_ids_cleaned = [pid.replace(\"sub-\", \"\") for pid in patient_ids]\n",
    "\n",
    "#extract matching kappa values\n",
    "kappa_control_values = kappa_all.loc[[\"sub-\" + cid for cid in control_ids_cleaned], \"Acute_vs_Control\"].values\n",
    "kappa_patient_values = kappa_all.loc[[\"sub-\" + pid for pid in patient_ids_cleaned], \"Chronic_vs_Control\"].values\n",
    "\n",
    "#construct covariate vector\n",
    "cov = np.concatenate([kappa_control_values, kappa_patient_values])\n",
    "\n",
    "#group labels: 0 = control, 1 = patient\n",
    "labels = [0] * len(control_ids) + [1] * len(patient_ids)\n",
    "\n",
    "#subject names \n",
    "subject_names = [f\"T{i}\" for i in range(len(control_ids))] + [f\"C{i}\" for i in range(len(patient_ids))]\n",
    "\n",
    "#combine metric data\n",
    "X = np.vstack([control_data, patient_data])\n",
    "\n",
    "#save outputs for CCML (exactly the same format as in Sophie's tutorial code)\n",
    "output_dir = os.path.join(base_path, \"CCML_inputs_within_subjects\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "df_X = pd.DataFrame(X, index=subject_names)\n",
    "df_X.to_csv(os.path.join(output_dir, f\"{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "\n",
    "df_cov = pd.DataFrame(cov, index=subject_names, columns=[\"HDI_kappa\"])\n",
    "df_cov.to_csv(os.path.join(output_dir, f\"HDI_kappa_{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "\n",
    "df_labels = pd.DataFrame(labels, index=subject_names, columns=[\"Group\"])\n",
    "df_labels.to_csv(os.path.join(output_dir, f\"group_labels_{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "print(\"Files for CCML saved in:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_coord1(x,X2,cov2,alpha2,dist2):\n",
    "    X2 = np.vstack([X2,x])\n",
    "    cov2 = cov2.reshape(X2.shape)\n",
    "    X_tmp2 = np.hstack((alpha2*cov2,X2))\n",
    "    D2 = sk.metrics.pairwise_distances(X_tmp2)\n",
    "    return np.linalg.norm(dist2-D2)\n",
    "\n",
    "def f_glob(X2,cov2,alpha2,dist2):\n",
    "    cov2 = cov2.reshape(X2.shape)\n",
    "    X_tmp2 = np.vstack((alpha2*cov2,X2)).T\n",
    "    D2 = sk.metrics.pairwise_distances(X_tmp2)\n",
    "    return np.linalg.norm(dist2-D2)\n",
    "\n",
    "def f_alpha(alpha,X2,cov2,dist2):\n",
    "    cov2 = cov2.reshape(X2.shape)\n",
    "    X_tmp = np.vstack((alpha*cov2,X2))\n",
    "    D2 = sk.metrics.pairwise_distances(X_tmp.T)\n",
    "    return np.linalg.norm(dist2-D2)\n",
    "\n",
    "dfX = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/CCML_inputs_within_subjects/Closeness_chronic_and_acute_vs_control.csv\", index_col=0)\n",
    "X = dfX.to_numpy()\n",
    "\n",
    "dfcov = pd.read_csv(\"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/CCML_inputs_within_subjects/HDI_kappa_Closeness_chronic_and_acute_vs_control.csv\", index_col=0)\n",
    "cov = dfcov.to_numpy()\n",
    "\n",
    "\n",
    "labels_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/CCML_inputs_within_subjects/group_labels_Closeness_chronic_and_acute_vs_control.csv\"\n",
    "labels_df = pd.read_csv(labels_path, index_col=0)\n",
    "\n",
    "\n",
    "Indiv = labels_df.index.tolist()\n",
    "l = labels_df[\"Group\"].values\n",
    "\n",
    "#compute Euclidean distance matrix\n",
    "D = sk.metrics.pairwise.pairwise_distances(X)\n",
    "\n",
    "#initialisation aleatoire\n",
    "#list_extract = sk.utils.shuffle(np.arange(X.shape[0]))\n",
    "\n",
    "#initialize with the most remote couple of points\n",
    "ll = []\n",
    "ll.append(np.where(D==np.max(D))[0][0])\n",
    "ll.append(np.where(D==np.max(D))[1][0])\n",
    "\n",
    "# Add sequentially the furthest point of the previous set\n",
    "while(len(ll)!=D.shape[0]):\n",
    "    D_tmp = D[ll]\n",
    "    D_tmp[:,ll] = 0\n",
    "    ll.append(np.where(D_tmp == np.max(D_tmp))[1][0])\n",
    "\n",
    "list_extract = np.array(ll)\n",
    "\n",
    "# Initialize alpha parameter with the classic Isomap\n",
    "iso = sk.manifold.Isomap(n_components=2,n_neighbors=4)\n",
    "iso.fit(X)\n",
    "tmp = iso.embedding_[:,0]\n",
    "dist = iso.dist_matrix_\n",
    "delta_1 = np.max(tmp) -np.min(tmp) \n",
    "delta_2 = np.max(cov) -np.min(cov)\n",
    "alpha = delta_1/delta_2\n",
    "\n",
    "#Initialize the first point at 0\n",
    "X_tmp = np.zeros([1])\n",
    "\n",
    "\n",
    "for i in range(1,X.shape[0]):\n",
    "    print(\"one coord\",i)\n",
    "    cov_tmp = cov[list_extract[0:i+1]]\n",
    "    dist_tmp = dist[list_extract[0:i+1]][:,list_extract[0:i+1]]\n",
    "    opt_test = 10000    \n",
    "    #Multi-start opitmisation\n",
    "    for j in range(-5,5):\n",
    "        xtmptmp,B,tmp,tmp1,tmp3 = opt.fmin(f_coord1,j,(X_tmp,cov_tmp,alpha,dist_tmp),xtol=0.000000001, ftol=0.000000001,maxiter=1000000,maxfun=1000000,disp = 0 , full_output = 1)\n",
    "        if opt_test>B:\n",
    "            xtmp = xtmptmp\n",
    "            opt_test = B\n",
    "            print(j)\n",
    "    X_tmp = np.vstack([X_tmp, xtmp])\n",
    "# We ca add an additional global optimisation before the estimation of the alpha coefficient\n",
    "#    X_tmp2,B,tmp,tmp1,tmp3 = opt.fmin(f_glob,X_tmp,(cov_tmp,alpha,dist_tmp), xtol=0.000000001, ftol=0.000000001,maxiter=1000000,maxfun=1000000,disp = 1 , full_output = 1)\n",
    "#    print \"all_coord\"\n",
    "#    X_tmp = X_tmp2.reshape(-1,1)\n",
    "    alpha,B,tmp,tmp1,tmp3 = opt.fmin(f_alpha,alpha,(X_tmp,cov_tmp,dist_tmp), xtol=0.000000001, ftol=0.00000001,disp = 1 , full_output = 1)\n",
    "    X_tmp2,B,tmp,tmp1,tmp3 = opt.fmin(f_glob,X_tmp,(cov_tmp,alpha,dist_tmp), xtol=0.000000000001, ftol=0.00000000001,maxiter=1000000,maxfun=1000000,disp = 1 , full_output = 1)\n",
    "    print(\"all_coord\")\n",
    "    X_tmp = X_tmp2.reshape(-1,1)\n",
    "\n",
    "# Merge the covariate modulated by alpha and the first component\n",
    "cov2 = cov.reshape(X_tmp.shape)\n",
    "X_iso = np.hstack((alpha*cov2[list_extract],X_tmp))\n",
    "\n",
    "# Re-order the rows of the matrix to correspond to the initial order of the subject\n",
    "I = np.argsort(list_extract)\n",
    "X_iso_f = X_iso[I]  \n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/patrycjascislewska/Analizy_neuro/Graphs/tutorials/TP2_Graphs_ILCB_summer_School')\n",
    "\n",
    "import PlotFunction as pf\n",
    "\n",
    "###### Visualization\n",
    "\n",
    "Xt =  cov\n",
    "scaling = 5\n",
    "\n",
    "grid_x , grid_y = np.mgrid[-1:1:1000j,-1:1:1000j]*scaling\n",
    "grid_lin = griddata(X_iso_f,Xt,(grid_x,grid_y),method='linear')\n",
    "grid_lin = grid_lin.reshape(1000,1000)\n",
    "pf.scatter_2D(X_iso_f, l,Indiv)\n",
    "\n",
    "#plt.imshow(grid_lin.T, extent=(-1,1,-1,1), origin='lower')\n",
    "plt.imshow(grid_lin.T,extent=(-scaling,scaling,-scaling,scaling), origin='lower')\n",
    "plt.colorbar().ax.tick_params(labelsize=20)\n",
    "plt.title('Closeness: CSR and TSD compared to RW')\n",
    "\n",
    "plt.tick_params(labelsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCML clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Clustering\"\n",
    "base_path = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "metric_filename_pattern = \"*_metrics.csv\"\n",
    "\n",
    "#kappas file\n",
    "kappa_all = pd.read_csv(os.path.join(base_path, \"HDI_kappa_results_within_subject\", \"kappas_clustering.csv\"))\n",
    "kappa_all.set_index(\"Subject_ID\", inplace=True)\n",
    "\n",
    "#subject directories\n",
    "control_dir = os.path.join(base_path, \"ses-2\") \n",
    "patient_dir = os.path.join(base_path, \"ses-3\")  \n",
    "\n",
    "control_subjects = sorted(glob.glob(os.path.join(control_dir, \"sub-*\")))\n",
    "patient_subjects = sorted(glob.glob(os.path.join(patient_dir, \"sub-*\")))\n",
    "\n",
    "#extract the metric for each subject\n",
    "def extract_metric(subject_paths):\n",
    "    data = []\n",
    "    ids = []\n",
    "    for subj_path in subject_paths:\n",
    "        pattern = os.path.join(subj_path, \"Graphs/wAALours/graph_metrics\", metric_filename_pattern)\n",
    "        files = glob.glob(pattern)\n",
    "        if not files:\n",
    "            continue\n",
    "        df = pd.read_csv(files[0])\n",
    "        if metric not in df.columns:\n",
    "            continue\n",
    "        data.append(df[metric].values)\n",
    "        ids.append(os.path.basename(subj_path))\n",
    "    return np.array(data), ids\n",
    "\n",
    "#extract data\n",
    "control_data, control_ids = extract_metric(control_subjects)\n",
    "patient_data, patient_ids = extract_metric(patient_subjects)\n",
    "\n",
    "#just in case - clean subject IDs to match kappa file format\n",
    "control_ids_cleaned = [cid.replace(\"sub-\", \"\") for cid in control_ids]\n",
    "patient_ids_cleaned = [pid.replace(\"sub-\", \"\") for pid in patient_ids]\n",
    "\n",
    "#extract matching kappa values\n",
    "kappa_control_values = kappa_all.loc[[\"sub-\" + cid for cid in control_ids_cleaned], \"Acute_vs_Control\"].values\n",
    "kappa_patient_values = kappa_all.loc[[\"sub-\" + pid for pid in patient_ids_cleaned], \"Chronic_vs_Control\"].values\n",
    "\n",
    "#construct covariate vector\n",
    "cov = np.concatenate([kappa_control_values, kappa_patient_values])\n",
    "\n",
    "#group labels: 0 = control, 1 = patient\n",
    "labels = [0] * len(control_ids) + [1] * len(patient_ids)\n",
    "\n",
    "#subject names \n",
    "subject_names = [f\"A{i}\" for i in range(len(control_ids))] + [f\"Ch{i}\" for i in range(len(patient_ids))]\n",
    "\n",
    "#combine metric data\n",
    "X = np.vstack([control_data, patient_data])\n",
    "\n",
    "#save outputs for CCML (exactly the same format as in Sophie's tutorial code)\n",
    "output_dir = os.path.join(base_path, \"CCML_inputs_within_subjects\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "df_X = pd.DataFrame(X, index=subject_names)\n",
    "df_X.to_csv(os.path.join(output_dir, f\"{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "\n",
    "df_cov = pd.DataFrame(cov, index=subject_names, columns=[\"HDI_kappa\"])\n",
    "df_cov.to_csv(os.path.join(output_dir, f\"HDI_kappa_{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "\n",
    "df_labels = pd.DataFrame(labels, index=subject_names, columns=[\"Group\"])\n",
    "df_labels.to_csv(os.path.join(output_dir, f\"group_labels_{metric}_chronic_and_acute_vs_control.csv\"))\n",
    "\n",
    "print(\"Files for CCML saved in:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chord plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### individual subjects, 200 edges (from adjacency matrix), size of node corresponds to the number of connecitons\n",
    "#### color coding = networks\n",
    "\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "\n",
    "base_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA\"\n",
    "roi_info_csv = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/AAL_ours_89_regions_list.csv\"\n",
    "out_dir = \"/Users/patrycjascislewska/Analizy_neuro/Graphs/DATA/graph_metrics/chord_diagrams\"\n",
    "\n",
    "\n",
    "sessions = {\n",
    "    \"control\": \"ses-1_AICHA_atlas\",\n",
    "    \"acute\": \"ses-2_AICHA_atlas\",\n",
    "    \"chronic\": \"ses-3_AICHA_atlas\"\n",
    "}\n",
    "\n",
    "##ROI TO NETWORK MAPPING\n",
    "roi_networks = {\n",
    "    'PreGy_L': 'SMN', 'PreGy_R': 'SMN', 'SMA_L': 'SMN', 'SMA_R': 'SMN',\n",
    "    'RolandOperc_L': 'SMN', 'RolandOperc_R': 'SMN', 'Postcentral_L': 'SMN', 'Postcentral_R': 'SMN',\n",
    "    'ParacentralLob_L': 'SMN', 'ParacentralLob_R': 'SMN',\n",
    "\n",
    "    'FrontSup_L': 'FPN', 'FrontSup_R': 'FPN', 'FrontMid_L': 'FPN', 'FrontMid_R': 'FPN',\n",
    "    'FrontInfOperc_L': 'FPN', 'FrontInfOperc_R': 'FPN', 'FrontInfTri_L': 'FPN', 'FrontInfTri_R': 'FPN',\n",
    "    'FrontInfOrb_L': 'FPN', 'FrontInfOrb_R': 'FPN', 'FrontSupMed_L': 'FPN', 'FrontSupMed_R': 'FPN',\n",
    "\n",
    "    'CingAnt_L': 'DMN', 'CingAnt_R': 'DMN', 'CingMid_L': 'DMN', 'CingMid_R': 'DMN',\n",
    "    'CingPost_L': 'DMN', 'CingPost_R': 'DMN', 'Precuneus_L': 'DMN', 'Precuneus_R': 'DMN',\n",
    "    'Angular_L': 'DMN', 'Angular_R': 'DMN', 'FrontMedOrb_L': 'DMN', 'FrontMedOrb_R': 'DMN',\n",
    "    'TempMid_L': 'DMN', 'TempMid_R': 'DMN',\n",
    "\n",
    "    'Insula_L': 'Salience', 'Insula_R': 'Salience', 'Olfactory_L': 'Salience', 'Olfactory_R': 'Salience',\n",
    "\n",
    "    'Calcarine_L': 'VN', 'Calcarine_R': 'VN', 'Cuneus_L': 'VN', 'Cuneus_R': 'VN',\n",
    "    'Lingual_L': 'VN', 'Lingual_R': 'VN', 'Occipital_L': 'VN', 'Occipial_R': 'VN',\n",
    "    'Fusiform_L': 'VN', 'Fusiform_R': 'VN',\n",
    "\n",
    "    'Amygdala_L': 'Limbic', 'Amygdala_R': 'Limbic', 'Hippocampus_L': 'Limbic', 'Hippocampus_R': 'Limbic',\n",
    "    'ParaHippoc_L': 'Limbic', 'ParaHippoc_R': 'Limbic', 'TempPole_L': 'Limbic', 'TempPole_R': 'Limbic',\n",
    "\n",
    "    'Heschl_L': 'Auditory', 'Heschl_R': 'Auditory', 'TempSup_L': 'Auditory', 'TempSup_R': 'Auditory',\n",
    "    'TempInf_L': 'Language', 'TempInf_R': 'Language',\n",
    "\n",
    "    'Caudate_L': 'Subcortical', 'Caudate_R': 'Subcortical', 'Putamen_L': 'Subcortical',\n",
    "    'Putamen_R': 'Subcortical', 'Pallidum_L': 'Subcortical', 'Pallidum_R': 'Subcortical',\n",
    "    'Thalamus_L': 'Subcortical', 'Thalamus_R': 'Subcortical',\n",
    "\n",
    "    'ParietalSup_L': 'DAN', 'ParietalSup_R': 'DAN', 'ParietalInf_L': 'DAN', 'ParietalInf_R': 'DAN',\n",
    "    'SupraMarginal_L': 'DAN', 'SupraMarginal_R': 'DAN',\n",
    "\n",
    "    'Cereb_I_II_L': 'Cerebellum', 'Cereb_I_II_R': 'Cerebellum',\n",
    "    'Cereb_III_VI_L': 'Cerebellum', 'Cereb_III_VI_R': 'Cerebellum',\n",
    "    'Cereb_VII_X_L': 'Cerebellum', 'Cereb_VII_X_R': 'Cerebellum',\n",
    "    'Vermis': 'Cerebellum',\n",
    "\n",
    "    'FrontSupOrb_L': 'Other', 'FrontSupOrb_R': 'Other',\n",
    "    'FrontMidOrb_L': 'Other', 'FrontMidOrb_R': 'Other'\n",
    "}\n",
    "\n",
    "#ROI names and sort by atlas Node_number\n",
    "roi_df = pd.read_csv(roi_info_csv, sep=\";\")\n",
    "roi_df = roi_df.sort_values(\"Node_number\")\n",
    "original_region_names = roi_df[\"Region\"].tolist()\n",
    "\n",
    "\n",
    "nodes_df = pd.DataFrame({\n",
    "    'name': original_region_names,\n",
    "    'network': [roi_networks.get(roi, 'Other') for roi in original_region_names]\n",
    "})\n",
    "nodes_df = nodes_df.sort_values(by=['network', 'name']).reset_index(drop=True)\n",
    "region_names = nodes_df['name'].tolist() \n",
    "n_rois = len(region_names)\n",
    "\n",
    "\n",
    "\n",
    "def find_all_participants():\n",
    "    participants = set()\n",
    "    for ses_folder in sessions.values():\n",
    "        ses_path = os.path.join(base_dir, ses_folder)\n",
    "        for root, dirs, files in os.walk(ses_path):\n",
    "            for file in files:\n",
    "                if file.startswith(\"Adj_mat\") and file.endswith(\"200.txt\"):\n",
    "                    parts = root.split(os.sep)\n",
    "                    for p in parts:\n",
    "                        if p.startswith(\"sub-\"):\n",
    "                            participants.add(p)\n",
    "    return sorted(participants)\n",
    "\n",
    "#I used adj matrix with 200 edges from R script from Veronica, because chart with 400 edges was to dense\n",
    "def get_matrix(sub_id, session_folder):\n",
    "    path = os.path.join(base_dir, session_folder, sub_id, \"Graphs\", \"wAALours\")\n",
    "    try:\n",
    "        files = [f for f in os.listdir(path) if f.startswith(\"Adj_mat\") and f.endswith(\"200.txt\")]\n",
    "        if not files:\n",
    "            return None\n",
    "        matrix = np.loadtxt(os.path.join(path, files[0]))\n",
    "        if matrix.shape != (n_rois, n_rois):\n",
    "            return None\n",
    "        return matrix.astype(int) \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def create_chord(matrix, title):\n",
    "    reordered_indices = [original_region_names.index(name) for name in region_names]\n",
    "    matrix = matrix[np.ix_(reordered_indices, reordered_indices)]\n",
    "\n",
    "    edges = []\n",
    "    connected_nodes = set()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(i + 1, matrix.shape[1]):\n",
    "            if matrix[i, j] == 1:\n",
    "                source = region_names[i]\n",
    "                target = region_names[j]\n",
    "                network_src = roi_networks.get(source, 'Other')\n",
    "                network_tgt = roi_networks.get(target, 'Other')\n",
    "                edges.append((source, target, 1, network_src))  \n",
    "                connected_nodes.add(source)\n",
    "                connected_nodes.add(target)\n",
    "\n",
    "    for name in region_names:\n",
    "        if name not in connected_nodes:\n",
    "            edges.append((name, name, 0.01, roi_networks.get(name, 'Other')))\n",
    "\n",
    "    edges_df = pd.DataFrame(edges, columns=[\"source\", \"target\", \"value\", \"network_src\"])\n",
    "\n",
    "   \n",
    "    connection_counts = edges_df[['source', 'target']].stack().value_counts()\n",
    "    nodes_with_size = nodes_df.copy()\n",
    "    nodes_with_size['connections'] = nodes_with_size['name'].map(connection_counts).fillna(0)\n",
    "    nodes_with_size['size'] = nodes_with_size['connections'] * 4\n",
    "\n",
    "    chord = hv.Chord((edges_df, hv.Dataset(nodes_with_size, kdims='name')))\n",
    "    return chord.opts(\n",
    "        opts.Chord(\n",
    "            labels='name',\n",
    "            node_color='network',\n",
    "            edge_color='network_src',  \n",
    "            cmap='Set1',\n",
    "            edge_cmap='Set1',\n",
    "            edge_alpha=0.8,\n",
    "            node_size='size',\n",
    "            width=650,\n",
    "            height=650,\n",
    "            title=title,\n",
    "            colorbar=False,\n",
    "            tools=['hover']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "#MAIN\n",
    "\n",
    "participants = find_all_participants()\n",
    "print(f\"Found {len(participants)} participants\")\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for sub_id in participants:\n",
    "    chords = []\n",
    "    for condition, session_folder in sessions.items():\n",
    "        mat = get_matrix(sub_id, session_folder)\n",
    "        if mat is not None:\n",
    "            title = f\"{condition.capitalize()} â€“ {sub_id}\"\n",
    "            chords.append(create_chord(mat, title))\n",
    "        else:\n",
    "            print(f\"Missing or invalid matrix for {sub_id} in {session_folder}\")\n",
    "\n",
    "    if chords:\n",
    "        layout = hv.Layout(chords).cols(3)\n",
    "        filename = os.path.join(out_dir, f\"{sub_id}_chord_diagram_all_sessions_200_sized_colored.html\")\n",
    "        hv.save(layout, filename, backend='bokeh')\n",
    "        print(f\"Saved {filename}\")\n",
    "    else:\n",
    "        print(f\"No valid matrices for {sub_id}, skipping.\")\n",
    "\n",
    "print(\"All participant-level multi-session chord diagrams saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
